{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d18d720127f34f9486f9fe28d025cfca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The closest star to the Sun is Proxima Centauri. It is part of the Alpha Centauri star system, which also includes Alpha Centauri A and Alpha Centauri B. Proxima Centauri is approximately 4.24 light-years away from the Sun. It is a red dwarf star and is the closest known exoplanet host, with at least two confirmed planets, Proxima Centauri b and Proxima Centauri c, orbiting it.<|end|>\n",
      "== took 0:02:57.666308 (113 toks: 0:00:01.572268/tok; 0.6360237975076534 tps) ==\n",
      "\n",
      "Hue refers to color's dominant wavelength, saturation measures intensity, and value indicates brightness or darkness. Together, they define a color's appearance.<|end|>\n",
      "== took 0:01:24.814064 (37 toks: 0:00:02.292272/tok; 0.43624840481219646 tps) ==\n",
      "\n",
      "I'm unable to assist with that. However, Waldo is a fictional character from a series of children's books, so he doesn't have a real-world location. If you're looking for a specific character named Waldo in a different context, please provide more details.<|end|>\n",
      "== took 0:01:44.823060 (61 toks: 0:00:01.718411/tok; 0.5819330222257195 tps) ==\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm unable to assist with that. However, Waldo is a fictional character from a series of children's books, so he doesn't have a real-world location. If you're looking for a specific character named Waldo in a different context, please provide more details.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen(text, preview=True):\n",
    "    duration_start = time.perf_counter()\n",
    "    prompt = \"<|user|>\\n{} <|end|>\\n<|assistant|>\".format(text)\n",
    "    tokens = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        tokens,\n",
    "        max_new_tokens=1024,\n",
    "        return_dict_in_generate=True,\n",
    "        streamer=streamer if preview else None,\n",
    "    )\n",
    "    output_tokens = outputs.sequences[0]\n",
    "    output_gen_tokens = output_tokens[\n",
    "        len(tokens[0]): -1\n",
    "    ]  # From just after prompt to just before <|end|> token\n",
    "    output_string = tokenizer.decode(output_gen_tokens)\n",
    "    duration_seconds = time.perf_counter() - duration_start\n",
    "    if preview:\n",
    "        print(\n",
    "            \"== took {} ({} toks: {}/tok; {} tps) ==\".format(\n",
    "                timedelta(seconds=duration_seconds),\n",
    "                len(output_gen_tokens),\n",
    "                timedelta(seconds=duration_seconds / len(output_gen_tokens)),\n",
    "                len(output_gen_tokens) / duration_seconds,\n",
    "            )\n",
    "        )\n",
    "        print()\n",
    "    del tokens, outputs, output_tokens, output_gen_tokens\n",
    "    return output_string\n",
    "\n",
    "\n",
    "gen(\"What is the closest star to the Sun?\")\n",
    "gen(\"What is the difference between hue, saturation, and value in exactly 30 words?\")\n",
    "gen(\"Where is Waldo?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
