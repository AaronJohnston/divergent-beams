{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1be13b0c-a2d0-48ef-81d0-d8a1fbe461e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (4.41.2)\n",
      "Requirement already satisfied: accelerate in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.30.1)\n",
      "Requirement already satisfied: optimum in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.20.0)\n",
      "Requirement already satisfied: nvidia-ml-py in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (12.535.133)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (3.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.23.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (2.1.0)\n",
      "Requirement already satisfied: coloredlogs in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from optimum) (15.0.1)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from optimum) (1.12)\n",
      "Requirement already satisfied: datasets in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from optimum) (2.19.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers[sentencepiece]<4.42.0,>=4.26.0->optimum) (0.2.0)\n",
      "Requirement already satisfied: protobuf in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers[sentencepiece]<4.42.0,>=4.26.0->optimum) (4.25.3)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from coloredlogs->optimum) (10.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->optimum) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->optimum) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->optimum) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->optimum) (1.5.3)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->optimum) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->optimum) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->optimum) (3.9.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->optimum) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (4.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets->optimum) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets->optimum) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets->optimum) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers accelerate optimum nvidia-ml-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b3eb5e6-2476-4b97-a0bf-304847b4c83a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7c3356e1f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.utils import is_flash_attn_2_available\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from datetime import timedelta\n",
    "import time\n",
    "from collections import namedtuple\n",
    "import json\n",
    "\n",
    "torch.random.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd16d255-3252-4d0f-9b1b-e7c0ea85c6a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "\n",
    "def check_gpu(step):\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"{step}: GPU memory used: {info.used // 1024**2} MB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df44f2c5-6f19-44ec-a5a7-b82164330a60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def D(obj):\n",
    "    if isinstance(obj, tuple):\n",
    "        print(len(obj))\n",
    "    elif isinstance(obj, torch.Tensor):\n",
    "        print(obj.shape)\n",
    "        display(obj)\n",
    "    else:\n",
    "        display(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b26136ca-dd13-48d6-a6d5-b6c1106df5d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23307b476aa4e02866ed02c64d14dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda\n",
      "model init: GPU memory used: 7813 MB.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True,\n",
    "    use_cache=True,\n",
    "    # attn_implementation='flash_attention_2',\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print('device', device)\n",
    "\n",
    "check_gpu('model init')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37365508-011a-47db-96af-2617ad2f759e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_candidates = 16\n",
    "max_new_tokens = 3\n",
    "batch_size = 8\n",
    "p_falloff = 0.5 # UNIMPLEMENTED\n",
    "prune_similar_sequences = True # UNIMPLEMENTED\n",
    "prune_similar_branches = True # UNIMPLEMENTED\n",
    "prune_similar_embeddings = True # UNIMPLEMENTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a537c32-7c28-449a-b82a-37da1a247416",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 18])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 32010,  1724,   338,   278,  1556,  5972,  2078,   287,   310,\n",
       "         11203, 29973, 29871, 32007, 32001,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 18])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False]],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidates init: GPU memory used: 7843 MB.\n"
     ]
    }
   ],
   "source": [
    "def init_candidates(text: str):\n",
    "    prompt = \"<|user|>\\n{} <|end|>\\n<|assistant|>\".format(text)\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "\n",
    "    max_total_tokens = inputs.input_ids.shape[1] + max_new_tokens\n",
    "\n",
    "    # (max_candidates, max_total_tokens)\n",
    "    candidates = torch.zeros((max_candidates, max_total_tokens), dtype=torch.long, device=device)\n",
    "    # (max_candidates, max_total_tokens)\n",
    "    candidate_masks = torch.zeros((max_candidates, max_total_tokens), dtype=torch.bool, device=device)\n",
    "    # (max_candidates)\n",
    "    candidate_parents = torch.zeros((max_candidates), dtype=torch.long, device=device)\n",
    "    # (max_candidates)\n",
    "    candidate_logprobs = torch.zeros((max_candidates), dtype=torch.float32, device=device)\n",
    "\n",
    "    candidates[0, :inputs.input_ids.shape[1]] = inputs.input_ids\n",
    "    candidate_masks[0, :inputs.input_ids.shape[1]] = inputs.attention_mask\n",
    "    candidate_parents[0] = 0\n",
    "    candidate_logprobs[0] = 0.0\n",
    "\n",
    "    return candidates, candidate_masks, candidate_parents, candidate_logprobs\n",
    "\n",
    "candidates, candidate_masks, candidate_parents, candidate_logprobs = init_candidates('What is the most popular breed of dog?')\n",
    "D(candidates)\n",
    "D(candidate_masks)\n",
    "D(candidate_parents)\n",
    "D(candidate_logprobs)\n",
    "\n",
    "check_gpu('candidates init')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84a09268-495a-4b87-bd59-b31bcb374caf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test addl inputs added: GPU memory used: 7843 MB.\n"
     ]
    }
   ],
   "source": [
    "# For testing batch inputs\n",
    "inputs2 = tokenizer(\"A dog\", return_tensors='pt')\n",
    "candidates[11, :inputs2.input_ids.shape[1]] = inputs2.input_ids\n",
    "candidate_masks[11, :inputs2.input_ids.shape[1]] = inputs2.attention_mask\n",
    "candidate_parents[11] = 0\n",
    "candidate_logprobs[11] = -1.3\n",
    "\n",
    "check_gpu('test addl inputs added')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0159a1d-c1fd-48ce-826a-e85849e3d78a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[32000, 32000, 32000, 32000,     1,  1714,   319],\n",
       "        [32000,     1,  1714,   350,   607,   338,  5520],\n",
       "        [    1,  1714,   315,   607,   338,  1584,  5520]]), 'attention_mask': tensor([[0, 0, 0, 0, 1, 1, 1],\n",
       "        [0, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_be = tokenizer([\"String A\", \"String B which is longer\", \"String C which is even longer\"], return_tensors=\"pt\", padding=True)\n",
    "test_be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d584923-06bc-41a6-8ccc-e909108593a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|endoftext|><|endoftext|><|endoftext|><|endoftext|><s> String A',\n",
       " '<|endoftext|><s> String B which is longer',\n",
       " '<s> String C which is even longer']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(test_be.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fdc2ad3-30dd-4f3a-9f7c-32447b083181",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infer start: GPU memory used: 7843 MB.\n",
      "torch.Size([8, 18])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 32010,  1724,   338,   278,  1556,  5972,  2078,   287,   310,\n",
       "         11203, 29973, 29871, 32007, 32001,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 18])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False]],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch views made: GPU memory used: 7843 MB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 18, 32064])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.8125,  1.3438, -0.4473,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 4.2812,  9.6875, 10.1250,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 6.0625,  2.9844,  3.8281,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 1.5859, -1.2031, -3.5625,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.0703,  0.7109, -5.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 3.5938, -3.8750, -3.3281,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch forward run: GPU memory used: 8035 MB.\n",
      "batch outputs deleted: GPU memory used: 8035 MB.\n",
      "torch.Size([8, 18])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    1,   319, 11203,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 18])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False]],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch views made: GPU memory used: 8035 MB.\n",
      "torch.Size([8, 18, 32064])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch forward run: GPU memory used: 8035 MB.\n",
      "batch outputs deleted: GPU memory used: 8035 MB.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all batches run: GPU memory used: 8035 MB.\n"
     ]
    }
   ],
   "source": [
    "def infer(candidates, candidate_masks, candidate_parents, candidate_logprobs):\n",
    "    with torch.inference_mode():\n",
    "        batches = (max_candidates + batch_size - 1) // batch_size  # Round up to nearest whole number of batches\n",
    "\n",
    "        check_gpu('infer start')\n",
    "        for i in range(0, batches, 1):\n",
    "            batch_candidates = candidates[i * batch_size:(i + 1) * batch_size]\n",
    "            D(batch_candidates)\n",
    "            batch_candidate_masks = candidate_masks[i * batch_size:(i + 1) * batch_size]\n",
    "            D(batch_candidate_masks)\n",
    "\n",
    "            check_gpu('batch views made')\n",
    "\n",
    "            batch_outputs = model(input_ids=batch_candidates, attention_mask=batch_candidate_masks)\n",
    "            D(batch_outputs.logits)\n",
    "\n",
    "            # Possibly turn off caching to save memory here?\n",
    "            check_gpu('batch forward run')\n",
    "\n",
    "            del batch_outputs\n",
    "\n",
    "            check_gpu('batch outputs deleted')\n",
    "\n",
    "outputs = infer(candidates, candidate_masks, candidate_parents, candidate_logprobs)\n",
    "D(outputs)\n",
    "\n",
    "check_gpu('all batches run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca8be196-eac7-459a-bcf9-64fe0979ace4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def top_p_tokens(logits, top_p=0.9):\n",
    "    \"\"\"logits of shape (batch_size, vocab_size)\"\"\"\n",
    "    with torch.inference_mode():\n",
    "    \n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)\n",
    "        sorted_probs = F.softmax(sorted_logits, dim=-1)\n",
    "        cum_probs = torch.cumsum(probs, dim=-1)\n",
    "        # Create tensor of bools indicating which indices are cumulatively less than top_p\n",
    "        sorted_keep_indices = cum_probs < 0.9\n",
    "        # Keep the last element that went over top_p\n",
    "        sorted_keep_indices[1:] = sorted_keep_indices[:-1].clone()\n",
    "        sorted_keep_indices[0] = 1  # Always keep the first element\n",
    "        keep_toks = sorted_indices[sorted_keep_indices]\n",
    "        keep_probs = probs[sorted_keep_indices]\n",
    "        return keep_toks, keep_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17c3bba1-ed50-4c27-8d71-8912665bd666",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def candidates_generator(text: str):\n",
    "#     print(text)\n",
    "#     candidates, candidate_masks, candidate_parents, candidate_logprobs = _init_candidates(text)\n",
    "\n",
    "#     return candidates, candidate_masks, candidate_parents, candidate_logprobs\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
