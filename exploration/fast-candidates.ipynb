{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1be13b0c-a2d0-48ef-81d0-d8a1fbe461e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (4.41.2)\n",
      "Requirement already satisfied: accelerate in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.31.0)\n",
      "Requirement already satisfied: optimum in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.20.0)\n",
      "Requirement already satisfied: nvidia-ml-py in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (12.535.133)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (3.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.23.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (2.1.0)\n",
      "Requirement already satisfied: coloredlogs in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from optimum) (15.0.1)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from optimum) (1.12)\n",
      "Requirement already satisfied: datasets in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from optimum) (2.19.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers[sentencepiece]<4.42.0,>=4.26.0->optimum) (0.2.0)\n",
      "Requirement already satisfied: protobuf in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers[sentencepiece]<4.42.0,>=4.26.0->optimum) (4.25.3)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from coloredlogs->optimum) (10.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->optimum) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->optimum) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->optimum) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->optimum) (1.5.3)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->optimum) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->optimum) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->optimum) (3.9.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->optimum) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (4.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets->optimum) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets->optimum) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets->optimum) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers accelerate optimum nvidia-ml-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b3eb5e6-2476-4b97-a0bf-304847b4c83a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa7df5da1f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.utils import is_flash_attn_2_available\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from datetime import timedelta\n",
    "import time\n",
    "from collections import namedtuple\n",
    "import json\n",
    "\n",
    "torch.random.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd16d255-3252-4d0f-9b1b-e7c0ea85c6a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "\n",
    "def check_gpu(step):\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"{step}: GPU memory used: {info.used // 1024**2} MB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "df44f2c5-6f19-44ec-a5a7-b82164330a60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def D(obj, label=None, c=True):\n",
    "    print()\n",
    "    if label:\n",
    "        print(label)\n",
    "        \n",
    "    if isinstance(obj, tuple):\n",
    "        print(len(obj))\n",
    "    elif isinstance(obj, torch.Tensor):\n",
    "        print(obj.shape)\n",
    "        if c: # Contents\n",
    "            display(obj)\n",
    "    else:\n",
    "        if c: # Contents\n",
    "            display(obj)\n",
    "            \n",
    "def DS(obj, label=None):\n",
    "    D(obj, label, c=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b26136ca-dd13-48d6-a6d5-b6c1106df5d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f851748b594f20be6416f346c90125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda\n",
      "model init: GPU memory used: 7813 MB.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True,\n",
    "    use_cache=True,\n",
    "    # attn_implementation='flash_attention_2',\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print('device', device)\n",
    "\n",
    "check_gpu('model init')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37365508-011a-47db-96af-2617ad2f759e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_candidates = 16\n",
    "max_new_tokens = 3\n",
    "batch_size = 8\n",
    "p_falloff = 0.5 # UNIMPLEMENTED\n",
    "prune_similar_sequences = True # UNIMPLEMENTED\n",
    "prune_similar_branches = True # UNIMPLEMENTED\n",
    "prune_similar_embeddings = True # UNIMPLEMENTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a537c32-7c28-449a-b82a-37da1a247416",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 18])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 32010,  1724,   338,   278,  1556,  5972,  2078,   287,   310,\n",
       "         11203, 29973, 29871, 32007, 32001,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 18])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False]],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidates init: GPU memory used: 7843 MB.\n"
     ]
    }
   ],
   "source": [
    "def init_candidates(text: str):\n",
    "    prompt = \"<|user|>\\n{} <|end|>\\n<|assistant|>\".format(text)\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "\n",
    "    max_total_tokens = inputs.input_ids.shape[1] + max_new_tokens\n",
    "\n",
    "    # (max_candidates, max_total_tokens)\n",
    "    candidates = torch.zeros((max_candidates, max_total_tokens), dtype=torch.long, device=device)\n",
    "    # (max_candidates, max_total_tokens)\n",
    "    candidate_masks = torch.zeros((max_candidates, max_total_tokens), dtype=torch.bool, device=device)\n",
    "    # (max_candidates)\n",
    "    candidate_parents = torch.zeros((max_candidates), dtype=torch.long, device=device)\n",
    "    # (max_candidates)\n",
    "    candidate_logprobs = torch.zeros((max_candidates), dtype=torch.float32, device=device)\n",
    "\n",
    "    candidates[0, :inputs.input_ids.shape[1]] = inputs.input_ids\n",
    "    candidate_masks[0, :inputs.input_ids.shape[1]] = inputs.attention_mask\n",
    "    candidate_parents[0] = 0\n",
    "    candidate_logprobs[0] = 0.0\n",
    "\n",
    "    return candidates, candidate_masks, candidate_parents, candidate_logprobs\n",
    "\n",
    "candidates, candidate_masks, candidate_parents, candidate_logprobs = init_candidates('What is the most popular breed of dog?')\n",
    "D(candidates)\n",
    "D(candidate_masks)\n",
    "D(candidate_parents)\n",
    "D(candidate_logprobs)\n",
    "\n",
    "check_gpu('candidates init')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84a09268-495a-4b87-bd59-b31bcb374caf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test addl inputs added: GPU memory used: 7843 MB.\n"
     ]
    }
   ],
   "source": [
    "# For testing batch inputs\n",
    "inputs2 = tokenizer(\"<|user|>\\n{} <|end|>\\n<|assistant|>\".format('What is the most popular breed of cat?'), return_tensors='pt')\n",
    "candidates[11, :inputs2.input_ids.shape[1]] = inputs2.input_ids\n",
    "candidate_masks[11, :inputs2.input_ids.shape[1]] = inputs2.attention_mask\n",
    "candidate_parents[11] = 0\n",
    "candidate_logprobs[11] = -1.3\n",
    "\n",
    "check_gpu('test addl inputs added')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0159a1d-c1fd-48ce-826a-e85849e3d78a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[32000, 32000, 32000, 32000,     1,  1714,   319],\n",
       "        [32000,     1,  1714,   350,   607,   338,  5520],\n",
       "        [    1,  1714,   315,   607,   338,  1584,  5520]]), 'attention_mask': tensor([[0, 0, 0, 0, 1, 1, 1],\n",
       "        [0, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_be = tokenizer([\"String A\", \"String B which is longer\", \"String C which is even longer\"], return_tensors=\"pt\", padding=True)\n",
    "test_be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d584923-06bc-41a6-8ccc-e909108593a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|endoftext|><|endoftext|><|endoftext|><|endoftext|><s> String A',\n",
       " '<|endoftext|><s> String B which is longer',\n",
       " '<s> String C which is even longer']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(test_be.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1fdc2ad3-30dd-4f3a-9f7c-32447b083181",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infer start: GPU memory used: 8165 MB.\n",
      "torch.Size([8, 18])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 32010,  1724,   338,   278,  1556,  5972,  2078,   287,   310,\n",
       "         11203, 29973, 29871, 32007, 32001,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "torch.Size([8, 18])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False]],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "batch views made: GPU memory used: 8165 MB.\n",
      "torch.Size([8, 18, 32064])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.8125,  1.3438, -0.4473,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 4.2812,  9.6875, 10.1250,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 6.0625,  2.9844,  3.8281,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 1.5859, -1.2031, -3.5625,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.0703,  0.7109, -5.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 3.5938, -3.8750, -3.3281,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "batch forward run: GPU memory used: 8165 MB.\n",
      "batch outputs deleted: GPU memory used: 8165 MB.\n",
      "torch.Size([8, 18, 32064])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.8125,  1.3438, -0.4473,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 4.2812,  9.6875, 10.1250,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 6.0625,  2.9844,  3.8281,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 1.5859, -1.2031, -3.5625,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.0703,  0.7109, -5.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 3.5938, -3.8750, -3.3281,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all batches run: GPU memory used: 8165 MB.\n"
     ]
    }
   ],
   "source": [
    "def infer(candidates, candidate_masks, candidate_parents, candidate_logprobs):\n",
    "    with torch.inference_mode():\n",
    "        batches = (max_candidates + batch_size - 1) // batch_size  # Round up to nearest whole number of batches\n",
    "\n",
    "        check_gpu('infer start')\n",
    "        for i in range(0, batches, 1):\n",
    "            batch_candidates = candidates[i * batch_size:(i + 1) * batch_size]\n",
    "            D(batch_candidates)\n",
    "            batch_candidate_masks = candidate_masks[i * batch_size:(i + 1) * batch_size]\n",
    "            D(batch_candidate_masks)\n",
    "\n",
    "            check_gpu('batch views made')\n",
    "\n",
    "            batch_outputs = model(input_ids=batch_candidates, attention_mask=batch_candidate_masks)\n",
    "            D(batch_outputs.logits)\n",
    "\n",
    "            # Possibly turn off caching to save memory here?\n",
    "            check_gpu('batch forward run')\n",
    "\n",
    "            # del batch_outputs\n",
    "\n",
    "            check_gpu('batch outputs deleted')\n",
    "            break\n",
    "            \n",
    "        return batch_outputs\n",
    "\n",
    "logits = infer(candidates, candidate_masks, candidate_parents, candidate_logprobs).logits\n",
    "D(logits)\n",
    "\n",
    "check_gpu('all batches run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca8be196-eac7-459a-bcf9-64fe0979ace4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 32064])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[9.9975e-01, 9.6088e-05, 8.4797e-05,  ..., 9.9887e-20, 5.6914e-20,\n",
       "         3.2429e-20],\n",
       "        [3.3339e-02, 1.7845e-02, 8.1702e-03,  ..., 4.3864e-09, 3.6364e-09,\n",
       "         3.3110e-09],\n",
       "        [3.3339e-02, 1.7845e-02, 8.1702e-03,  ..., 4.3864e-09, 3.6364e-09,\n",
       "         3.3110e-09],\n",
       "        ...,\n",
       "        [3.3339e-02, 1.7845e-02, 8.1702e-03,  ..., 4.3864e-09, 3.6364e-09,\n",
       "         3.3110e-09],\n",
       "        [3.3339e-02, 1.7845e-02, 8.1702e-03,  ..., 4.3864e-09, 3.6364e-09,\n",
       "         3.3110e-09],\n",
       "        [3.3339e-02, 1.7845e-02, 8.1702e-03,  ..., 4.3864e-09, 3.6364e-09,\n",
       "         3.3110e-09]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 32064])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.9998, 0.9998, 0.9999,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        [0.0333, 0.0512, 0.0594,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        [0.0333, 0.0512, 0.0594,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        ...,\n",
       "        [0.0333, 0.0512, 0.0594,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        [0.0333, 0.0512, 0.0594,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        [0.0333, 0.0512, 0.0594,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Actually, no attention mask is needed -- all candidates will always be the same number of tokens (having started from the same\n",
    "# base and with the same number of generations), so all we have to do is feed a view of the candidates tensor with just valid tokens\n",
    "# into the model). Separately keep track of length of candidate sequences.\n",
    "\n",
    "# def top_p_tokens(logits, top_p=0.9):\n",
    "#     \"\"\"logits of shape (batch_size, curr_seq_len, vocab_size)\"\"\"\n",
    "#     with torch.inference_mode():\n",
    "last_tok_logits = logits[:, -1, :]\n",
    "\n",
    "sorted_logits, sorted_indices = torch.sort(last_tok_logits, descending=True, dim=-1)\n",
    "sorted_probs = F.softmax(sorted_logits, dim=-1)\n",
    "D(sorted_probs)\n",
    "cum_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "D(cum_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ef51c88-e3e1-4fe3-94fb-753f582b6e2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 32064])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False,  ..., False, False, False],\n",
       "        [ True,  True,  True,  ..., False, False, False],\n",
       "        [ True,  True,  True,  ..., False, False, False],\n",
       "        ...,\n",
       "        [ True,  True,  True,  ..., False, False, False],\n",
       "        [ True,  True,  True,  ..., False, False, False],\n",
       "        [ True,  True,  True,  ..., False, False, False]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create tensor of bools indicating which indices are cumulatively less than top_p\n",
    "keep_indices = cum_probs < 0.9\n",
    "\n",
    "# Keep the last element that went over top_p\n",
    "keep_indices[:, 1:] = keep_indices[:, :-1].clone() # Is this inefficient?\n",
    "keep_indices[:, 0] = 1  # Always keep the first element\n",
    "\n",
    "D(keep_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf5b48ba-5d74-453b-98d3-e5b7f185fc10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([90805])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([29871, 24278, 26785,  ..., 15108, 15268, 16121], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([90805])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([9.9975e-01, 3.3339e-02, 1.7845e-02,  ..., 1.4587e-05, 1.4587e-05,\n",
       "        1.4587e-05], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "keep_toks = sorted_indices[keep_indices]\n",
    "keep_probs = sorted_probs[keep_indices]\n",
    "\n",
    "D(keep_toks)\n",
    "D(keep_probs)\n",
    "\n",
    "# top_p_tokens(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "44a5a01d-1ebe-4437-88ae-d0626bb2dcf6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([90805, 18])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 32010,  1724,  ...,     0,     0,     0],\n",
       "        [    0,     0,     0,  ...,     0,     0,     0],\n",
       "        [    0,     0,     0,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    0,     0,     0,  ...,     0,     0,     0],\n",
       "        [    0,     0,     0,  ...,     0,     0,     0],\n",
       "        [    0,     0,     0,  ...,     0,     0,     0]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "D(candidates.index_select(0, keep_indices.nonzero()[:, 0])) # COMPONENT A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c78a6b8d-b1cc-4119-9ca0-18011e9869e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([90805])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([29871, 24278, 26785,  ..., 15108, 15268, 16121], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "D(sorted_indices[keep_indices]) # I think this is COMPONENT B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f0d7981f-cd53-4285-a8d4-2ef9ed501e85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([90805])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([9.9975e-01, 3.3339e-02, 1.7845e-02,  ..., 1.4587e-05, 1.4587e-05,\n",
       "        1.4587e-05], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "D(sorted_probs[keep_indices]) # I think this is COMPONENT C, still needs to be ln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "88c98df2-ffd2-422e-822f-625df356c415",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input_ids\n",
      "torch.Size([1, 15])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 32010,  1724,   338,   278,  1556,  5972,  2078,   287,   310,\n",
       "         11203, 29973, 29871, 32007, 32001]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s><|user|> What is the most popular breed of dog? <|end|><|assistant|>']\n",
      "after init candidates: GPU memory used: 9823 MB.\n",
      "\n",
      "candidates\n",
      "torch.Size([1, 15])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 32010,  1724,   338,   278,  1556,  5972,  2078,   287,   310,\n",
       "         11203, 29973, 29871, 32007, 32001]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "candidate_parents\n",
      "torch.Size([1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "candidate_logprobs\n",
      "torch.Size([1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "num_batches 1\n",
      "infer start: GPU memory used: 9823 MB.\n",
      "\n",
      "batch_candidates\n",
      "torch.Size([1, 15])\n",
      "\n",
      "batch_candidate_parents\n",
      "torch.Size([1])\n",
      "\n",
      "batch_candidate_logprobs\n",
      "torch.Size([1])\n",
      "\n",
      "batch_logits\n",
      "torch.Size([1, 15, 32064])\n",
      "\n",
      "last_tok_logits\n",
      "torch.Size([1, 32064])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[6.9062, 4.6250, 3.8438,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sorted_logits\n",
      "torch.Size([1, 32064])\n",
      "\n",
      "sorted_indices\n",
      "torch.Size([1, 32064])\n",
      "\n",
      "sorted_probs\n",
      "torch.Size([1, 32064])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[8.0513e-01, 1.3991e-01, 5.1470e-02,  ..., 1.0126e-21, 5.7697e-22,\n",
       "         8.3120e-23]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.0000], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cum_probs\n",
      "torch.Size([1, 32064])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.8051, 0.9450, 0.9965,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "keep_indices\n",
      "torch.Size([1, 32064])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  ..., False, False, False]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "carryover_candidate_indices\n",
      "torch.Size([3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "carryover_candidates\n",
      "torch.Size([3, 15])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 32010,  1724,   338,   278,  1556,  5972,  2078,   287,   310,\n",
       "         11203, 29973, 29871, 32007, 32001],\n",
       "        [    1, 32010,  1724,   338,   278,  1556,  5972,  2078,   287,   310,\n",
       "         11203, 29973, 29871, 32007, 32001],\n",
       "        [    1, 32010,  1724,   338,   278,  1556,  5972,  2078,   287,   310,\n",
       "         11203, 29973, 29871, 32007, 32001]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "carryover_candidate_parents\n",
      "torch.Size([3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "carryover_candidate_logprobs\n",
      "torch.Size([3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new_candidate_toks\n",
      "torch.Size([3, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1094],\n",
       "        [ 450],\n",
       "        [7579]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new_candidate_tok_logprobs\n",
      "torch.Size([3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.2168, -1.9668, -2.9668], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new_candidates\n",
      "torch.Size([3, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 32010,  1724,   338,   278,  1556,  5972,  2078,   287,   310,\n",
       "         11203, 29973, 29871, 32007, 32001,  1094],\n",
       "        [    1, 32010,  1724,   338,   278,  1556,  5972,  2078,   287,   310,\n",
       "         11203, 29973, 29871, 32007, 32001,   450],\n",
       "        [    1, 32010,  1724,   338,   278,  1556,  5972,  2078,   287,   310,\n",
       "         11203, 29973, 29871, 32007, 32001,  7579]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new_candidate_logprobs\n",
      "torch.Size([3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.2168, -1.9668, -2.9668], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infer - after batch run: GPU memory used: 9823 MB.\n",
      "infer end: GPU memory used: 9823 MB.\n",
      "\n",
      "new_candidates\n",
      "torch.Size([3, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 32010,  1724,   338,   278,  1556,  5972,  2078,   287,   310,\n",
       "         11203, 29973, 29871, 32007, 32001,  1094],\n",
       "        [    1, 32010,  1724,   338,   278,  1556,  5972,  2078,   287,   310,\n",
       "         11203, 29973, 29871, 32007, 32001,   450],\n",
       "        [    1, 32010,  1724,   338,   278,  1556,  5972,  2078,   287,   310,\n",
       "         11203, 29973, 29871, 32007, 32001,  7579]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s><|user|> What is the most popular breed of dog? <|end|><|assistant|> As', '<s><|user|> What is the most popular breed of dog? <|end|><|assistant|> The', '<s><|user|> What is the most popular breed of dog? <|end|><|assistant|> According']\n",
      "\n",
      "new_candidate_parents\n",
      "torch.Size([3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new_candidate_logprobs\n",
      "torch.Size([3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.2168, -1.9668, -2.9668], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================\n",
      "\n",
      "\n",
      "num_batches 1\n",
      "infer start: GPU memory used: 9823 MB.\n",
      "\n",
      "batch_candidates\n",
      "torch.Size([3, 16])\n",
      "\n",
      "batch_candidate_parents\n",
      "torch.Size([3])\n",
      "\n",
      "batch_candidate_logprobs\n",
      "torch.Size([3])\n",
      "\n",
      "batch_logits\n",
      "torch.Size([3, 16, 32064])\n",
      "\n",
      "last_tok_logits\n",
      "torch.Size([3, 32064])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3438, -5.3438, -8.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.8086, -2.1406, -5.2812,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 5.4688, -0.1816,  6.3125,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sorted_logits\n",
      "torch.Size([3, 32064])\n",
      "\n",
      "sorted_indices\n",
      "torch.Size([3, 32064])\n",
      "\n",
      "sorted_probs\n",
      "torch.Size([3, 32064])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[9.9596e-01, 3.5920e-03, 1.3928e-04,  ..., 6.7048e-22, 4.9053e-22,\n",
       "         1.3203e-22],\n",
       "        [9.4738e-01, 3.2418e-02, 8.1964e-03,  ..., 4.2003e-20, 3.9458e-20,\n",
       "         3.2712e-20],\n",
       "        [1.0000e+00, 3.6535e-08, 2.8453e-08,  ..., 6.5998e-24, 5.1399e-24,\n",
       "         2.7512e-24]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cum_probs\n",
      "torch.Size([3, 32064])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.9960, 0.9996, 0.9997,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        [0.9474, 0.9798, 0.9880,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "keep_indices\n",
      "torch.Size([3, 32064])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False,  ..., False, False, False],\n",
       "        [ True,  True, False,  ..., False, False, False],\n",
       "        [ True, False, False,  ..., False, False, False]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "carryover_candidate_indices\n",
      "torch.Size([4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 2], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "carryover_candidates\n",
      "torch.Size([4, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 32010,  1724,   338,   278,  1556,  5972,  2078,   287,   310,\n",
       "         11203, 29973, 29871, 32007, 32001,  1094],\n",
       "        [    1, 32010,  1724,   338,   278,  1556,  5972,  2078,   287,   310,\n",
       "         11203, 29973, 29871, 32007, 32001,   450],\n",
       "        [    1, 32010,  1724,   338,   278,  1556,  5972,  2078,   287,   310,\n",
       "         11203, 29973, 29871, 32007, 32001,   450],\n",
       "        [    1, 32010,  1724,   338,   278,  1556,  5972,  2078,   287,   310,\n",
       "         11203, 29973, 29871, 32007, 32001,  7579]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "carryover_candidate_parents\n",
      "torch.Size([4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "carryover_candidate_logprobs\n",
      "torch.Size([4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.2168, -1.9668, -1.9668, -2.9668], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new_candidate_toks\n",
      "torch.Size([4, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 310],\n",
       "        [1556],\n",
       "        [5972],\n",
       "        [ 304]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new_candidate_tok_logprobs\n",
      "torch.Size([4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.0040, -0.0541, -3.4291,  0.0000], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new_candidates\n",
      "torch.Size([4, 17])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 32010,  1724,   338,   278,  1556,  5972,  2078,   287,   310,\n",
       "         11203, 29973, 29871, 32007, 32001,  1094,   310],\n",
       "        [    1, 32010,  1724,   338,   278,  1556,  5972,  2078,   287,   310,\n",
       "         11203, 29973, 29871, 32007, 32001,   450,  1556],\n",
       "        [    1, 32010,  1724,   338,   278,  1556,  5972,  2078,   287,   310,\n",
       "         11203, 29973, 29871, 32007, 32001,   450,  5972],\n",
       "        [    1, 32010,  1724,   338,   278,  1556,  5972,  2078,   287,   310,\n",
       "         11203, 29973, 29871, 32007, 32001,  7579,   304]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new_candidate_logprobs\n",
      "torch.Size([4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.2208, -2.0208, -5.3958, -2.9668], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infer - after batch run: GPU memory used: 9823 MB.\n",
      "infer end: GPU memory used: 9823 MB.\n",
      "\n",
      "new_candidates\n",
      "torch.Size([4, 17])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 32010,  1724,   338,   278,  1556,  5972,  2078,   287,   310,\n",
       "         11203, 29973, 29871, 32007, 32001,  1094,   310],\n",
       "        [    1, 32010,  1724,   338,   278,  1556,  5972,  2078,   287,   310,\n",
       "         11203, 29973, 29871, 32007, 32001,   450,  1556],\n",
       "        [    1, 32010,  1724,   338,   278,  1556,  5972,  2078,   287,   310,\n",
       "         11203, 29973, 29871, 32007, 32001,   450,  5972],\n",
       "        [    1, 32010,  1724,   338,   278,  1556,  5972,  2078,   287,   310,\n",
       "         11203, 29973, 29871, 32007, 32001,  7579,   304]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s><|user|> What is the most popular breed of dog? <|end|><|assistant|> As of', '<s><|user|> What is the most popular breed of dog? <|end|><|assistant|> The most', '<s><|user|> What is the most popular breed of dog? <|end|><|assistant|> The popular', '<s><|user|> What is the most popular breed of dog? <|end|><|assistant|> According to']\n",
      "\n",
      "new_candidate_parents\n",
      "torch.Size([4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new_candidate_logprobs\n",
      "torch.Size([4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.2208, -2.0208, -5.3958, -2.9668], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def init_candidates(text: str):\n",
    "    prompt = \"<|user|>\\n{} <|end|>\\n<|assistant|>\".format(text)\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    D(inputs.input_ids, 'input_ids')\n",
    "    print(tokenizer.batch_decode(inputs.input_ids))\n",
    "\n",
    "    candidates = inputs.input_ids.to(device)\n",
    "    candidate_parents = torch.zeros((1), dtype=torch.long, device=device)\n",
    "    candidate_logprobs = torch.zeros((1), dtype=torch.float32, device=device)\n",
    "\n",
    "    return candidates, candidate_parents, candidate_logprobs\n",
    "\n",
    "def top_p_single_batch(logits, candidates, candidate_parents, candidate_logprobs):\n",
    "    last_tok_logits = logits[:, -1, :]\n",
    "    D(last_tok_logits, 'last_tok_logits')\n",
    "    \n",
    "    sorted_logits, sorted_indices = torch.sort(last_tok_logits, descending=True, dim=-1)\n",
    "    DS(sorted_logits, 'sorted_logits')\n",
    "    DS(sorted_indices, 'sorted_indices')\n",
    "    sorted_probs = F.softmax(sorted_logits, dim=-1)\n",
    "    D(sorted_probs, 'sorted_probs')\n",
    "    display(sorted_probs.sum(dim=1))\n",
    "    cum_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "    D(cum_probs, 'cum_probs')\n",
    "    \n",
    "    # Create tensor of bools indicating which indices are cumulatively less than top_p\n",
    "    keep_indices = cum_probs < 0.96\n",
    "\n",
    "    # Keep the last element that went over top_p\n",
    "    keep_indices[:, 1:] = keep_indices[:, :-1].clone() # Is this inefficient?\n",
    "    keep_indices[:, 0] = 1  # Always keep the first element\n",
    "    D(keep_indices, 'keep_indices')\n",
    "    \n",
    "    carryover_candidate_indices = keep_indices.nonzero()[:, 0]\n",
    "    D(carryover_candidate_indices, 'carryover_candidate_indices')\n",
    "    \n",
    "    # OPTIM: Potential optimization -- have a fixed tensor of size (max_candidates, max_tokens) and copy this into that (batch-aware).\n",
    "    # OPTIM: consider which of these operations can be done in-place to prevent new allocations?\n",
    "    carryover_candidates = candidates.index_select(0, carryover_candidate_indices)\n",
    "    D(carryover_candidates, 'carryover_candidates')\n",
    "    \n",
    "    # Similar code could be used to trace entire origin of sequence. For now since server just traces parent of the preceding generation, not needed\n",
    "    # carryover_candidate_parents = candidate_parents.index_select(0, carryover_candidate_indices)  # Not strictly necessary since 1d\n",
    "    # D(carryover_candidate_parents, 'carryover_candidate_parents')\n",
    "    \n",
    "    carryover_candidate_logprobs = candidate_logprobs.index_select(0, carryover_candidate_indices)  # Not strictly necessary since 1d\n",
    "    D(carryover_candidate_logprobs, 'carryover_candidate_logprobs')\n",
    "    \n",
    "    new_candidate_toks = sorted_indices[keep_indices].unsqueeze(1)\n",
    "    D(new_candidate_toks, 'new_candidate_toks')\n",
    "    new_candidate_tok_logprobs = sorted_probs[keep_indices].log()\n",
    "    D(new_candidate_tok_logprobs, 'new_candidate_tok_logprobs')\n",
    "    \n",
    "    new_candidates = torch.cat([carryover_candidates, new_candidate_toks], dim=1)\n",
    "    D(new_candidates, 'new_candidates')\n",
    "    new_candidate_parents = carryover_candidate_parents\n",
    "    new_candidate_logprobs = carryover_candidate_logprobs.add_(new_candidate_tok_logprobs)\n",
    "    D(new_candidate_logprobs, 'new_candidate_logprobs')\n",
    "    \n",
    "    return new_candidates, new_candidate_parents, new_candidate_logprobs\n",
    "    \n",
    "\n",
    "def infer(candidates, candidate_parents, candidate_logprobs):\n",
    "    with torch.inference_mode():\n",
    "        num_batches = (candidates.shape[0] + batch_size - 1) // batch_size  # Round up to nearest whole number of batches\n",
    "        print('\\nnum_batches', num_batches)\n",
    "        new_candidates_list = []\n",
    "        new_candidate_parents_list = []\n",
    "        new_candidate_logprobs_list = []\n",
    "\n",
    "        check_gpu('infer start')\n",
    "        for i in range(0, num_batches, 1):\n",
    "            batch_candidates = candidates[i * batch_size:(i + 1) * batch_size]\n",
    "            DS(batch_candidates, 'batch_candidates')\n",
    "            batch_candidate_parents = candidate_parents[i * batch_size:(i + 1) * batch_size]\n",
    "            DS(batch_candidate_parents, 'batch_candidate_parents')\n",
    "            batch_candidate_logprobs = candidate_logprobs[i * batch_size:(i + 1) * batch_size]\n",
    "            DS(batch_candidate_logprobs, 'batch_candidate_logprobs')\n",
    "\n",
    "            batch_outputs = model(input_ids=batch_candidates)\n",
    "            DS(batch_outputs.logits, 'batch_logits')\n",
    "            \n",
    "            # TODO: Pruning step based on K-Means Clustering of embeddings here\n",
    "            \n",
    "            new_batch_candidates, new_batch_candidate_parents, new_batch_candidate_logprobs = top_p_single_batch(batch_outputs.logits, batch_candidates, batch_candidate_parents, batch_candidate_logprobs)\n",
    "            new_candidates_list.append(new_batch_candidates)\n",
    "            new_candidate_parents_list.append(new_batch_candidate_parents)\n",
    "            new_candidate_logprobs_list.append(new_batch_candidate_logprobs)\n",
    "            \n",
    "            # Possibly turn off caching to save memory here?\n",
    "            check_gpu('infer - after batch run')\n",
    "            \n",
    "        check_gpu('infer end')\n",
    "        return torch.cat(new_candidates_list), torch.cat(new_candidate_parents_list), torch.cat(new_candidate_logprobs_list)\n",
    "\n",
    "candidates, candidate_parents, candidate_logprobs = init_candidates('What is the most popular breed of dog?')\n",
    "check_gpu('after init candidates')\n",
    "D(candidates, 'candidates')\n",
    "D(candidate_parents, 'candidate_parents')\n",
    "D(candidate_logprobs, 'candidate_logprobs')\n",
    "new_candidates, new_candidate_parents, new_candidate_logprobs = infer(candidates, candidate_parents, candidate_logprobs)\n",
    "D(new_candidates, 'new_candidates')\n",
    "print(tokenizer.batch_decode(new_candidates))\n",
    "D(new_candidate_parents, 'new_candidate_parents')\n",
    "D(new_candidate_logprobs, 'new_candidate_logprobs')\n",
    "print()\n",
    "print('====================================')\n",
    "print()\n",
    "candidates = new_candidates\n",
    "candidate_parents = new_candidate_parents\n",
    "candidate_logprobs = new_candidate_logprobs\n",
    "new_candidates, new_candidate_parents, new_candidate_logprobs = infer(candidates, candidate_parents, candidate_logprobs)\n",
    "D(new_candidates, 'new_candidates')\n",
    "print(tokenizer.batch_decode(new_candidates))\n",
    "D(new_candidate_parents, 'new_candidate_parents')\n",
    "D(new_candidate_logprobs, 'new_candidate_logprobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc00638-8294-46be-af73-b05b57ba0a52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e17b31d3-df35-4577-b994-c60f368e706c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  ..., False, False, False], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "D(keep_indices.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa096391-6451-4e91-970d-c84f2b9d812a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1000, device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_indices.flatten()[0:1000].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9373bb1-c635-4554-b24a-6b7ed65022d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 32064])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[29871,   259, 29892,  ..., 22715, 25923, 24336],\n",
       "        [24278, 26785,  7066,  ..., 12645, 15084,  6941],\n",
       "        [24278, 26785,  7066,  ..., 12645, 15084,  6941],\n",
       "        ...,\n",
       "        [24278, 26785,  7066,  ..., 12645, 15084,  6941],\n",
       "        [24278, 26785,  7066,  ..., 12645, 15084,  6941],\n",
       "        [24278, 26785,  7066,  ..., 12645, 15084,  6941]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "D(sorted_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3cf60c92-adde-4f6f-aadc-d8e342b6fe2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 32064])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False,  ..., False, False, False],\n",
       "        [ True,  True,  True,  ..., False, False, False],\n",
       "        [ True,  True,  True,  ..., False, False, False],\n",
       "        ...,\n",
       "        [ True,  True,  True,  ..., False, False, False],\n",
       "        [ True,  True,  True,  ..., False, False, False],\n",
       "        [ True,  True,  True,  ..., False, False, False]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tensor(90805, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([    1, 12972, 12972, 12972, 12972, 12972, 12972, 12972],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D(keep_indices)\n",
    "print(keep_indices.sum())\n",
    "keep_indices.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef755d15-0de1-455b-b3bb-9015e349f440",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([90805])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([29871, 24278, 26785,  ..., 15108, 15268, 16121], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x = sorted_indices[keep_indices]\n",
    "D(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bdb67901-86d1-45f1-ad05-8acd3df68685",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2994, -0.1878,  1.9159,  0.6902, -2.3217],\n",
       "        [-1.1964,  0.1970, -1.1773,  0.1136,  1.1047],\n",
       "        [-1.3952,  0.4751, -0.8137,  0.9242, -0.2473],\n",
       "        [-1.4154,  0.9874, -1.4878,  0.5867,  0.1583],\n",
       "        [ 0.1102, -0.8188,  0.6328, -1.9169,  1.1711],\n",
       "        [ 0.0975,  0.9634,  0.8403, -1.2537,  0.9868],\n",
       "        [-0.4947, -1.2830,  0.9552,  1.2836, -0.6659],\n",
       "        [ 0.5651,  0.2877, -0.0334, -1.0619, -0.1144]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "torch.Size([8, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "torch.Size([8, 5, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1964,  0.1970, -1.1773,  0.1136,  1.1047],\n",
       "         [-0.2994, -0.1878,  1.9159,  0.6902, -2.3217],\n",
       "         [-1.1964,  0.1970, -1.1773,  0.1136,  1.1047],\n",
       "         [-1.1964,  0.1970, -1.1773,  0.1136,  1.1047],\n",
       "         [-1.1964,  0.1970, -1.1773,  0.1136,  1.1047]],\n",
       "\n",
       "        [[-1.1964,  0.1970, -1.1773,  0.1136,  1.1047],\n",
       "         [-1.1964,  0.1970, -1.1773,  0.1136,  1.1047],\n",
       "         [-1.1964,  0.1970, -1.1773,  0.1136,  1.1047],\n",
       "         [-1.1964,  0.1970, -1.1773,  0.1136,  1.1047],\n",
       "         [-1.1964,  0.1970, -1.1773,  0.1136,  1.1047]],\n",
       "\n",
       "        [[-1.1964,  0.1970, -1.1773,  0.1136,  1.1047],\n",
       "         [-1.1964,  0.1970, -1.1773,  0.1136,  1.1047],\n",
       "         [-1.1964,  0.1970, -1.1773,  0.1136,  1.1047],\n",
       "         [-1.1964,  0.1970, -1.1773,  0.1136,  1.1047],\n",
       "         [-1.1964,  0.1970, -1.1773,  0.1136,  1.1047]],\n",
       "\n",
       "        [[-1.1964,  0.1970, -1.1773,  0.1136,  1.1047],\n",
       "         [-1.1964,  0.1970, -1.1773,  0.1136,  1.1047],\n",
       "         [-1.1964,  0.1970, -1.1773,  0.1136,  1.1047],\n",
       "         [-1.1964,  0.1970, -1.1773,  0.1136,  1.1047],\n",
       "         [-1.1964,  0.1970, -1.1773,  0.1136,  1.1047]],\n",
       "\n",
       "        [[-1.1964,  0.1970, -1.1773,  0.1136,  1.1047],\n",
       "         [-1.1964,  0.1970, -1.1773,  0.1136,  1.1047],\n",
       "         [-1.1964,  0.1970, -1.1773,  0.1136,  1.1047],\n",
       "         [-1.1964,  0.1970, -1.1773,  0.1136,  1.1047],\n",
       "         [-1.1964,  0.1970, -1.1773,  0.1136,  1.1047]],\n",
       "\n",
       "        [[-1.1964,  0.1970, -1.1773,  0.1136,  1.1047],\n",
       "         [-1.1964,  0.1970, -1.1773,  0.1136,  1.1047],\n",
       "         [-1.1964,  0.1970, -1.1773,  0.1136,  1.1047],\n",
       "         [-1.1964,  0.1970, -1.1773,  0.1136,  1.1047],\n",
       "         [-1.1964,  0.1970, -1.1773,  0.1136,  1.1047]],\n",
       "\n",
       "        [[-1.1964,  0.1970, -1.1773,  0.1136,  1.1047],\n",
       "         [-1.1964,  0.1970, -1.1773,  0.1136,  1.1047],\n",
       "         [-1.1964,  0.1970, -1.1773,  0.1136,  1.1047],\n",
       "         [-1.1964,  0.1970, -1.1773,  0.1136,  1.1047],\n",
       "         [-1.1964,  0.1970, -1.1773,  0.1136,  1.1047]],\n",
       "\n",
       "        [[-1.1964,  0.1970, -1.1773,  0.1136,  1.1047],\n",
       "         [-1.1964,  0.1970, -1.1773,  0.1136,  1.1047],\n",
       "         [-1.1964,  0.1970, -1.1773,  0.1136,  1.1047],\n",
       "         [-1.1964,  0.1970, -1.1773,  0.1136,  1.1047],\n",
       "         [-1.1964,  0.1970, -1.1773,  0.1136,  1.1047]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(8, 5)\n",
    "D(a)\n",
    "b = torch.ones(8, 5, dtype=torch.long)\n",
    "b[0][1] = 0\n",
    "D(b)\n",
    "c = a[b]\n",
    "D(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6376f682-087c-44a7-82ef-704bbc3a39ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 2, 1],\n",
       "        [1, 2, 0]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([[10, 30, 20], [60, 40, 50]])\n",
    "sorted_idx = torch.argsort(t, dim=1)\n",
    "D(sorted_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "11ec5c13-edae-4d85-a77b-315b5c4b1770",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = np.arange(35).reshape(5, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "81d10805-5386-4167-ae44-6b56f5277b4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "b = x > 20\n",
    "D(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e5ef2457-aa59-41fb-9f96-3a25ac98a885",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False,  True,  True])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[:, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c9a38859-265e-4525-89dd-c032dbf4178b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5ebfdbb3-bd81-40b2-8abb-7d1528e2529f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21, 22, 23, 24, 25, 26, 27],\n",
       "       [28, 29, 30, 31, 32, 33, 34]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[b[:, 5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9eedfded-80cb-4640-9b9e-44853573540e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e22bcc6e-536a-412f-ad0b-035e88632265",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(5, 0), dtype=int64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, b[2, :]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130162f5-e83d-4f99-addf-2573e4946417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17c3bba1-ed50-4c27-8d71-8912665bd666",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def candidates_generator(text: str):\n",
    "#     print(text)\n",
    "#     candidates, candidate_masks, candidate_parents, candidate_logprobs = _init_candidates(text)\n",
    "\n",
    "#     return candidates, candidate_masks, candidate_parents, candidate_logprobs\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f485248f-9354-4690-b014-611989befd0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
