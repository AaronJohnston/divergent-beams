{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1be13b0c-a2d0-48ef-81d0-d8a1fbe461e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting accelerate\n",
      "  Downloading accelerate-0.31.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting optimum\n",
      "  Downloading optimum-1.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: nvidia-ml-py in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (12.535.133)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (3.13.3)\n",
      "Collecting huggingface-hub<1.0,>=0.23.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (2.1.0)\n",
      "Collecting coloredlogs (from optimum)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from optimum) (1.12)\n",
      "Collecting datasets (from optimum)\n",
      "  Downloading datasets-2.19.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Collecting sentencepiece!=0.1.92,>=0.1.91 (from transformers[sentencepiece]<4.42.0,>=4.26.0->optimum)\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: protobuf in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers[sentencepiece]<4.42.0,>=4.26.0->optimum) (4.25.3)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->optimum)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->optimum) (15.0.2)\n",
      "Collecting pyarrow-hotfix (from datasets->optimum)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->optimum) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->optimum) (1.5.3)\n",
      "Collecting requests (from transformers)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting xxhash (from datasets->optimum)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->optimum) (0.70.16)\n",
      "Collecting aiohttp (from datasets->optimum)\n",
      "  Using cached aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->optimum) (1.3.0)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets->optimum)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets->optimum)\n",
      "  Using cached frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->optimum)\n",
      "  Using cached multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets->optimum)\n",
      "  Using cached yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets->optimum)\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets->optimum) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets->optimum) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets->optimum) (1.16.0)\n",
      "Downloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading optimum-1.20.0-py3-none-any.whl (418 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m418.4/418.4 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.23.3-py3-none-any.whl (401 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.7/401.7 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (775 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m775.1/775.1 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.19.2-py3-none-any.whl (542 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.1/542.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "Using cached multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "Using cached yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "Installing collected packages: sentencepiece, xxhash, safetensors, requests, regex, pyarrow-hotfix, multidict, humanfriendly, frozenlist, async-timeout, yarl, huggingface-hub, coloredlogs, aiosignal, tokenizers, aiohttp, accelerate, transformers, datasets, optimum\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "Successfully installed accelerate-0.31.0 aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 coloredlogs-15.0.1 datasets-2.19.2 frozenlist-1.4.1 huggingface-hub-0.23.3 humanfriendly-10.0 multidict-6.0.5 optimum-1.20.0 pyarrow-hotfix-0.6 regex-2024.5.15 requests-2.32.3 safetensors-0.4.3 sentencepiece-0.2.0 tokenizers-0.19.1 transformers-4.41.2 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers accelerate optimum nvidia-ml-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b3eb5e6-2476-4b97-a0bf-304847b4c83a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9bfbbd24f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.utils import is_flash_attn_2_available\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from datetime import timedelta\n",
    "import time\n",
    "from collections import namedtuple\n",
    "import json\n",
    "\n",
    "torch.random.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd16d255-3252-4d0f-9b1b-e7c0ea85c6a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "\n",
    "def check_gpu(step):\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"{step}: GPU memory used: {info.used // 1024**2} MB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df44f2c5-6f19-44ec-a5a7-b82164330a60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def D(obj):\n",
    "    if isinstance(obj, tuple):\n",
    "        print(len(obj))\n",
    "    elif isinstance(obj, torch.Tensor):\n",
    "        print(obj.shape)\n",
    "        display(obj)\n",
    "    else:\n",
    "        display(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b26136ca-dd13-48d6-a6d5-b6c1106df5d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7964062e0b48828af994330e443d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/904 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6055b19584b24dec849e5bbd8996fbf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_phi3.py:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190feca1ff16489e83422fde09e705f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_phi3.py:   0%|          | 0.00/73.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b838f7933d744a55b1d10ed5a6b3d0a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/16.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7afb2b9ba1a49a29952411ebe8cfc9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eddc68010e3246ea88c717fe31b09ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f35662aab9d44b4afb4ea929e140ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a9208a078a44a18004cb55e1d6be58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5bc874a571749b4b43472a51e2dbec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/172 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cdbdf76485b4fc5aa19a5da546a674b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef874f8950d4eb59336adeb9d7e5716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c27bdd0b06344fdbbbf0277d0573b07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e67e39570d184be09a0bd8f4cacf0ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aea448ebd82d468aabea74811f67374e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/568 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda\n",
      "model init: GPU memory used: 7813 MB.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True,\n",
    "    use_cache=True,\n",
    "    # attn_implementation='flash_attention_2',\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print('device', device)\n",
    "\n",
    "check_gpu('model init')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37365508-011a-47db-96af-2617ad2f759e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_candidates = 16\n",
    "max_new_tokens = 3\n",
    "batch_size = 8\n",
    "p_falloff = 0.5 # UNIMPLEMENTED\n",
    "prune_similar_sequences = True # UNIMPLEMENTED\n",
    "prune_similar_branches = True # UNIMPLEMENTED\n",
    "prune_similar_embeddings = True # UNIMPLEMENTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a537c32-7c28-449a-b82a-37da1a247416",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 18])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 32010,  1724,   338,   278,  1556,  5972,  2078,   287,   310,\n",
       "         11203, 29973, 29871, 32007, 32001,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 18])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False]],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidates init: GPU memory used: 7843 MB.\n"
     ]
    }
   ],
   "source": [
    "def init_candidates(text: str):\n",
    "    prompt = \"<|user|>\\n{} <|end|>\\n<|assistant|>\".format(text)\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "\n",
    "    max_total_tokens = inputs.input_ids.shape[1] + max_new_tokens\n",
    "\n",
    "    # (max_candidates, max_total_tokens)\n",
    "    candidates = torch.zeros((max_candidates, max_total_tokens), dtype=torch.long, device=device)\n",
    "    # (max_candidates, max_total_tokens)\n",
    "    candidate_masks = torch.zeros((max_candidates, max_total_tokens), dtype=torch.bool, device=device)\n",
    "    # (max_candidates)\n",
    "    candidate_parents = torch.zeros((max_candidates), dtype=torch.long, device=device)\n",
    "    # (max_candidates)\n",
    "    candidate_logprobs = torch.zeros((max_candidates), dtype=torch.float32, device=device)\n",
    "\n",
    "    candidates[0, :inputs.input_ids.shape[1]] = inputs.input_ids\n",
    "    candidate_masks[0, :inputs.input_ids.shape[1]] = inputs.attention_mask\n",
    "    candidate_parents[0] = 0\n",
    "    candidate_logprobs[0] = 0.0\n",
    "\n",
    "    return candidates, candidate_masks, candidate_parents, candidate_logprobs\n",
    "\n",
    "candidates, candidate_masks, candidate_parents, candidate_logprobs = init_candidates('What is the most popular breed of dog?')\n",
    "D(candidates)\n",
    "D(candidate_masks)\n",
    "D(candidate_parents)\n",
    "D(candidate_logprobs)\n",
    "\n",
    "check_gpu('candidates init')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84a09268-495a-4b87-bd59-b31bcb374caf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test addl inputs added: GPU memory used: 7843 MB.\n"
     ]
    }
   ],
   "source": [
    "# For testing batch inputs\n",
    "inputs2 = tokenizer(\"<|user|>\\n{} <|end|>\\n<|assistant|>\".format('What is the most popular breed of cat?'), return_tensors='pt')\n",
    "candidates[11, :inputs2.input_ids.shape[1]] = inputs2.input_ids\n",
    "candidate_masks[11, :inputs2.input_ids.shape[1]] = inputs2.attention_mask\n",
    "candidate_parents[11] = 0\n",
    "candidate_logprobs[11] = -1.3\n",
    "\n",
    "check_gpu('test addl inputs added')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0159a1d-c1fd-48ce-826a-e85849e3d78a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[32000, 32000, 32000, 32000,     1,  1714,   319],\n",
       "        [32000,     1,  1714,   350,   607,   338,  5520],\n",
       "        [    1,  1714,   315,   607,   338,  1584,  5520]]), 'attention_mask': tensor([[0, 0, 0, 0, 1, 1, 1],\n",
       "        [0, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_be = tokenizer([\"String A\", \"String B which is longer\", \"String C which is even longer\"], return_tensors=\"pt\", padding=True)\n",
    "test_be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d584923-06bc-41a6-8ccc-e909108593a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|endoftext|><|endoftext|><|endoftext|><|endoftext|><s> String A',\n",
       " '<|endoftext|><s> String B which is longer',\n",
       " '<s> String C which is even longer']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(test_be.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fdc2ad3-30dd-4f3a-9f7c-32447b083181",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infer start: GPU memory used: 7843 MB.\n",
      "torch.Size([8, 18])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 32010,  1724,   338,   278,  1556,  5972,  2078,   287,   310,\n",
       "         11203, 29973, 29871, 32007, 32001,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 18])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False]],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch views made: GPU memory used: 7843 MB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 18, 32064])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.8125,  1.3438, -0.4473,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 4.2812,  9.6875, 10.1250,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 6.0625,  2.9844,  3.8281,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 1.5859, -1.2031, -3.5625,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.0703,  0.7109, -5.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 3.5938, -3.8750, -3.3281,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch forward run: GPU memory used: 8035 MB.\n",
      "batch outputs deleted: GPU memory used: 8035 MB.\n",
      "torch.Size([8, 18])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    1, 32010,  1724,   338,   278,  1556,  5972,  2078,   287,   310,\n",
       "          6635, 29973, 29871, 32007, 32001,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 18])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False]],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch views made: GPU memory used: 8035 MB.\n",
      "torch.Size([8, 18, 32064])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch forward run: GPU memory used: 8165 MB.\n",
      "batch outputs deleted: GPU memory used: 8165 MB.\n",
      "torch.Size([8, 18, 32064])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.2031, -0.1865, -1.1719,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all batches run: GPU memory used: 8165 MB.\n"
     ]
    }
   ],
   "source": [
    "def infer(candidates, candidate_masks, candidate_parents, candidate_logprobs):\n",
    "    with torch.inference_mode():\n",
    "        batches = (max_candidates + batch_size - 1) // batch_size  # Round up to nearest whole number of batches\n",
    "\n",
    "        check_gpu('infer start')\n",
    "        for i in range(0, batches, 1):\n",
    "            batch_candidates = candidates[i * batch_size:(i + 1) * batch_size]\n",
    "            D(batch_candidates)\n",
    "            batch_candidate_masks = candidate_masks[i * batch_size:(i + 1) * batch_size]\n",
    "            D(batch_candidate_masks)\n",
    "\n",
    "            check_gpu('batch views made')\n",
    "\n",
    "            batch_outputs = model(input_ids=batch_candidates, attention_mask=batch_candidate_masks)\n",
    "            D(batch_outputs.logits)\n",
    "\n",
    "            # Possibly turn off caching to save memory here?\n",
    "            check_gpu('batch forward run')\n",
    "\n",
    "            # del batch_outputs\n",
    "\n",
    "            check_gpu('batch outputs deleted')\n",
    "            \n",
    "        return batch_outputs\n",
    "\n",
    "logits = infer(candidates, candidate_masks, candidate_parents, candidate_logprobs).logits\n",
    "D(logits)\n",
    "\n",
    "check_gpu('all batches run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca8be196-eac7-459a-bcf9-64fe0979ace4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 32064])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0333, 0.0512, 0.0594,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        [0.0333, 0.0512, 0.0594,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        [0.0333, 0.0512, 0.0594,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        ...,\n",
       "        [0.0333, 0.0512, 0.0594,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        [0.0333, 0.0512, 0.0594,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        [0.0333, 0.0512, 0.0594,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 32064])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  ..., False, False, False],\n",
       "        [ True,  True,  True,  ..., False, False, False],\n",
       "        [ True,  True,  True,  ..., False, False, False],\n",
       "        ...,\n",
       "        [ True,  True,  True,  ..., False, False, False],\n",
       "        [ True,  True,  True,  ..., False, False, False],\n",
       "        [ True,  True,  True,  ..., False, False, False]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([90805])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([24278, 26785,  7066,  ..., 15108, 15268, 16121], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([90805])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([3.3339e-02, 1.7845e-02, 8.1702e-03,  ..., 1.4587e-05, 1.4587e-05,\n",
       "        1.4587e-05], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Actually, no attention mask is needed -- all candidates will always be the same number of tokens (having started from the same\n",
    "# base and with the same number of generations), so all we have to do is feed a view of the candidates tensor with just valid tokens\n",
    "# into the model). Separately keep track of length of candidate sequences.\n",
    "\n",
    "# def top_p_tokens(logits, top_p=0.9):\n",
    "#     \"\"\"logits of shape (batch_size, curr_seq_len, vocab_size)\"\"\"\n",
    "#     with torch.inference_mode():\n",
    "last_tok_logits = logits[:, -1, :]\n",
    "\n",
    "sorted_logits, sorted_indices = torch.sort(last_tok_logits, descending=True, dim=-1)\n",
    "sorted_probs = F.softmax(sorted_logits, dim=-1)\n",
    "cum_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "D(cum_probs)\n",
    "\n",
    "# Create tensor of bools indicating which indices are cumulatively less than top_p\n",
    "keep_indices = cum_probs < 0.9\n",
    "\n",
    "# Keep the last element that went over top_p\n",
    "keep_indices[:, 1:] = keep_indices[:, :-1].clone()\n",
    "keep_indices[:, 0] = 1  # Always keep the first element\n",
    "\n",
    "D(keep_indices)\n",
    "\n",
    "keep_toks = sorted_indices[keep_indices]\n",
    "keep_probs = sorted_probs[keep_indices]\n",
    "\n",
    "D(keep_toks)\n",
    "D(keep_probs)\n",
    "\n",
    "# top_p_tokens(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e17b31d3-df35-4577-b994-c60f368e706c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  ..., False, False, False], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "D(keep_indices.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa096391-6451-4e91-970d-c84f2b9d812a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1000, device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_indices.flatten()[0:1000].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3cf60c92-adde-4f6f-aadc-d8e342b6fe2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  ..., False, False, False],\n",
       "        [ True,  True,  True,  ..., False, False, False],\n",
       "        [ True,  True,  True,  ..., False, False, False],\n",
       "        ...,\n",
       "        [ True,  True,  True,  ..., False, False, False],\n",
       "        [ True,  True,  True,  ..., False, False, False],\n",
       "        [ True,  True,  True,  ..., False, False, False]], device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef755d15-0de1-455b-b3bb-9015e349f440",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([90805])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([24278, 26785,  7066,  ..., 15108, 15268, 16121], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "D(sorted_indices.flatten()[keep_indices.flatten()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17c3bba1-ed50-4c27-8d71-8912665bd666",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def candidates_generator(text: str):\n",
    "#     print(text)\n",
    "#     candidates, candidate_masks, candidate_parents, candidate_logprobs = _init_candidates(text)\n",
    "\n",
    "#     return candidates, candidate_masks, candidate_parents, candidate_logprobs\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
