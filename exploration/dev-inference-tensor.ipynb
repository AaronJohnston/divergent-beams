{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20264819-2d9c-40f4-9fcc-a2b7b019321c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (4.41.2)\n",
      "Requirement already satisfied: accelerate in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.31.0)\n",
      "Requirement already satisfied: optimum in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.20.0)\n",
      "Requirement already satisfied: nvidia-ml-py in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (12.535.133)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (3.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.23.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (2.1.0)\n",
      "Requirement already satisfied: coloredlogs in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from optimum) (15.0.1)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from optimum) (1.12)\n",
      "Requirement already satisfied: datasets in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from optimum) (2.19.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers[sentencepiece]<4.42.0,>=4.26.0->optimum) (0.2.0)\n",
      "Requirement already satisfied: protobuf in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers[sentencepiece]<4.42.0,>=4.26.0->optimum) (4.25.3)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from coloredlogs->optimum) (10.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->optimum) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->optimum) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->optimum) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->optimum) (1.5.3)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->optimum) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->optimum) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->optimum) (3.9.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->optimum) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (4.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets->optimum) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets->optimum) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets->optimum) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers accelerate optimum nvidia-ml-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2d0609e-c769-43dc-926b-f5b41beda157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f2acb86e1f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.utils import is_flash_attn_2_available\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from datetime import timedelta\n",
    "import time\n",
    "from collections import namedtuple\n",
    "import json\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "torch.random.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9112e61-a3e7-4ea5-853e-f27145f37e09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pynvml import *\n",
    "\n",
    "def check_gpu(step):\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"{step}: GPU memory used: {info.used // 1024**2} MB.\")\n",
    "    \n",
    "def D(obj, label=None, c=True):\n",
    "    print()\n",
    "    if label:\n",
    "        print(label)\n",
    "        \n",
    "    if isinstance(obj, tuple):\n",
    "        print(len(obj))\n",
    "    elif isinstance(obj, torch.Tensor) or isinstance(obj, np.ndarray):\n",
    "        print(obj.shape)\n",
    "        if c: # Contents\n",
    "            display(obj)\n",
    "    else:\n",
    "        if c: # Contents\n",
    "            display(obj)\n",
    "            \n",
    "def DS(obj, label=None):\n",
    "    D(obj, label, c=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0aa18d7-acb5-412f-8495-8483dbe71a61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InferenceTensor:\n",
    "    def __init__(self):\n",
    "        print('Initializing model...')\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map='auto',\n",
    "            trust_remote_code=True,\n",
    "            use_cache=True,\n",
    "            # attn_implementation='flash_attention_2',\n",
    "        )\n",
    "        print('Initializing tokenizer...')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"microsoft/Phi-3-mini-4k-instruct\")\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        self.batch_size = 8\n",
    "        \n",
    "    def candidates_generator(self, top_p: float, top_p_decay: float, top_k: float, max_beams: int, max_new_tokens: int, prompt: str):\n",
    "        candidates, candidate_logprobs = self._init_candidates(prompt)\n",
    "        for level_idx in range(max_new_tokens):\n",
    "            logits, embeddings = self._infer(candidates, candidate_logprobs)\n",
    "\n",
    "            self._farthest_neighbors(logits, embeddings, candidates, candidate_logprobs, max_beams)\n",
    "            \n",
    "            if candidates.shape[0] > max_beams:\n",
    "                start = time.perf_counter()\n",
    "                candidates, candidate_parents, candidate_aunts, candidate_logprobs, logits = self._k_means(logits, embeddings, candidates, candidate_logprobs, max_beams)\n",
    "                inference_duration = time.perf_counter() - start\n",
    "                print('K MEANS PRIOR {}: ({}) {} candidates, {} inference time, {} total time'.format(level_idx, time.perf_counter(), candidates.shape[0], inference_duration, time.perf_counter() - start))\n",
    "                yield self._format_k_means(level_idx, candidates, candidate_parents, candidate_aunts, candidate_logprobs, inference_duration)\n",
    "                print('K MEANS AFTER {}: ({}) {} candidates, {} inference time, {} total time'.format(level_idx, time.perf_counter(), candidates.shape[0], inference_duration, time.perf_counter() - start))\n",
    "\n",
    "            start = time.perf_counter()\n",
    "            candidates, candidate_parents, candidate_logprobs = self._top_p(logits, candidates, candidate_logprobs, top_p, top_k)\n",
    "            inference_duration = time.perf_counter() - start\n",
    "            print('TOP P PRIOR {}: ({}) {} candidates, {} inference time, {} total time'.format(level_idx, time.perf_counter(), candidates.shape[0], inference_duration, time.perf_counter() - start))\n",
    "            yield self._format_top_p(level_idx, candidates, candidate_parents, candidate_logprobs, inference_duration)\n",
    "            print('TOP P AFTER {}: ({}) {} candidates, {} inference time, {} total time'.format(level_idx, time.perf_counter(), candidates.shape[0], inference_duration, time.perf_counter() - start))\n",
    "            top_p *= top_p_decay\n",
    "\n",
    "        yield f\"event: message\\nid: END\\ndata: []\\n\\n\"\n",
    "\n",
    "    def _format_k_means(self, level_idx, candidates, candidate_parents, candidate_aunts, candidate_logprobs, duration):\n",
    "        candidate_texts = self.tokenizer.convert_ids_to_tokens(candidates[:, -1], skip_special_tokens=True)\n",
    "        candidate_probs = candidate_logprobs.exp()\n",
    "        candidate_dicts = []\n",
    "        idx = f\"{level_idx}-k\"\n",
    "        for i in range(len(candidate_texts)):\n",
    "            candidate_dicts.append({'content': candidate_texts[i], 'parent': candidate_parents[i], 'aunts': candidate_aunts[i], 'prob': candidate_probs[i].item()})\n",
    "        data = json.dumps({'id': idx, 'level_type': 'gather', 'duration': duration, 'nodes': candidate_dicts})\n",
    "        return f\"event: message\\nid: {idx}\\\"\\ndata: {data}\\n\\n\"\n",
    "\n",
    "    def _format_top_p(self, level_idx, candidates, candidate_parents, candidate_logprobs, duration):\n",
    "        candidate_texts = self.tokenizer.convert_ids_to_tokens(candidates[:, -1], skip_special_tokens=True)\n",
    "        candidate_probs = candidate_logprobs.exp()\n",
    "        candidate_dicts = []\n",
    "        idx = f\"{level_idx}-p\"\n",
    "        for i in range(len(candidate_texts)):\n",
    "            candidate_dicts.append({'content': candidate_texts[i], 'parent': candidate_parents[i], 'prob': candidate_probs[i].item()})\n",
    "        data = json.dumps({'id': idx, 'level_type': 'sample', 'duration': duration, 'nodes': candidate_dicts})\n",
    "        return f\"event: message\\nid: {idx}\\ndata: {data}\\n\\n\"\n",
    "\n",
    "\n",
    "    def _init_candidates(self, text: str):\n",
    "        prompt = \"<|user|>\\n{} <|end|>\\n<|assistant|>\".format(text)\n",
    "        inputs = self.tokenizer(prompt, return_tensors='pt')\n",
    "        D(inputs.input_ids, 'input_ids')\n",
    "\n",
    "        candidates = inputs.input_ids.to(self.device)\n",
    "        candidate_logprobs = torch.zeros((1), dtype=torch.float32, device=self.device)\n",
    "\n",
    "        return candidates, candidate_logprobs\n",
    "\n",
    "    def _k_means(self, logits, embeddings, candidates, candidate_logprobs, max_beams):\n",
    "        D(candidates, 'candidates')\n",
    "        D(candidate_logprobs, 'candidate_logprobs')\n",
    "        # === CPU ===\n",
    "        embeddings_np = embeddings.float().numpy(force=True)\n",
    "        D(embeddings_np, 'embeddings_np')\n",
    "        k_means = KMeans(n_clusters=min(max_beams, embeddings_np.shape[0]), random_state=0, n_init=\"auto\")\n",
    "        k_mean_space = k_means.fit_transform(embeddings_np)\n",
    "        D(k_mean_space, 'k_mean_space')\n",
    "        k_mean_clusters = k_means.predict(embeddings_np)\n",
    "        D(k_mean_clusters, 'k_mean_clusters')\n",
    "        k_mean_logprob_mass = np.log(np.bincount(k_mean_clusters, weights=candidate_logprobs.cpu().exp()))\n",
    "        D(k_mean_logprob_mass, 'k_mean_logprob_mass')\n",
    "        closest = np.argmin(k_mean_space, axis=0)\n",
    "        D(closest, 'closest')\n",
    "        # === END CPU ===\n",
    "        \n",
    "        closest_indices = torch.from_numpy(closest).to(self.device)\n",
    "        new_candidates = candidates.index_select(0, closest_indices)\n",
    "        D(new_candidates, 'new_candidates')\n",
    "        new_candidate_parents = closest_indices.tolist()\n",
    "        D(new_candidate_parents, 'new_candidate_parents')\n",
    "        new_candidate_aunts = [torch.nonzero(torch.from_numpy(k_mean_clusters).to(self.device) == i).squeeze(-1).tolist() for i in range(new_candidates.shape[0])]\n",
    "        D(new_candidate_aunts, 'new_candidate_aunts')\n",
    "        new_candidate_logprobs = torch.from_numpy(k_mean_logprob_mass).to(self.device)\n",
    "        D(new_candidate_logprobs, 'new_candidate_logprobs')\n",
    "        new_candidate_logits = logits.index_select(0, closest_indices)\n",
    "        \n",
    "        return new_candidates, new_candidate_parents, new_candidate_aunts, new_candidate_logprobs, new_candidate_logits\n",
    "        \n",
    "    def _farthest_neighbors(self, logits, embeddings, candidates, candidate_logprobs, max_beams):\n",
    "        print('FARTHEST NEIGHBORS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
    "        D(candidates, 'candidates')\n",
    "        D(candidate_logprobs, 'candidate_logprobs')\n",
    "        \n",
    "        selected = torch.zeros((candidates.shape[0],), dtype=torch.bool)\n",
    "        max_prob_idx = candidate_logprobs.argmax()\n",
    "        selected[max_prob_idx] = 1\n",
    "        \n",
    "        D(selected, 'selected')\n",
    "        \n",
    "        for idx in range(max_beams - 1):\n",
    "            selected_embeddings = embeddings[selected]\n",
    "            D(selected_embeddings, 'selected_embeddings')\n",
    "            selected_embeddings_view = selected_embeddings.view((1, selected_embeddings.shape[1], selected_embeddings[0]))\n",
    "            D(selected_embeddings_view, 'selected_embeddings_view')\n",
    "            d = F.cosine_similarity(candidate_logprobs, candidate_logprobs[selected], dim=1)\n",
    "            D(d, 'd')\n",
    "            \n",
    "                \n",
    "\n",
    "\n",
    "    def _top_p(self, logits, candidates, candidate_logprobs, top_p, top_k):\n",
    "        D(candidates, 'candidates')\n",
    "        D(candidate_logprobs, 'candidate_logprobs')\n",
    "        \n",
    "        last_tok_logits = logits[:, -1, :]\n",
    "        D(last_tok_logits, 'last_tok_logits')\n",
    "\n",
    "        sorted_logits, sorted_indices = torch.sort(last_tok_logits, descending=True, dim=-1)\n",
    "        DS(sorted_logits, 'sorted_logits')\n",
    "        DS(sorted_indices, 'sorted_indices')\n",
    "        sorted_probs = F.softmax(sorted_logits, dim=-1)\n",
    "        D(sorted_probs, 'sorted_probs')\n",
    "        display(sorted_probs.sum(dim=1))\n",
    "        cum_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "        D(cum_probs, 'cum_probs')\n",
    "\n",
    "        # Create tensor of bools indicating which indices are cumulatively less than top_p\n",
    "        keep_indices = cum_probs < top_p\n",
    "\n",
    "        # Keep the last element that went over top_p\n",
    "        keep_indices[:, 1:] = keep_indices[:, :-1].clone() # Is this inefficient?\n",
    "        keep_indices[:, 0] = 1  # Always keep the first element\n",
    "        D(keep_indices, 'keep_indices')\n",
    "\n",
    "        # Don't keep any indices that are greater than top_k\n",
    "        keep_indices[:, top_k:] = 0\n",
    "        D(keep_indices, 'keep_indices after top_k')\n",
    "\n",
    "        new_candidate_parents = keep_indices.nonzero()[:, 0]\n",
    "        D(new_candidate_parents, 'new_candidate_parents')\n",
    "\n",
    "        # OPTIM: Potential optimization -- have a fixed tensor of size (max_candidates, max_tokens) and copy this into that (batch-aware).\n",
    "        # OPTIM: consider which of these operations can be done in-place to prevent new allocations?\n",
    "        carryover_candidates = candidates.index_select(0, new_candidate_parents)\n",
    "        D(carryover_candidates, 'carryover_candidates')\n",
    "\n",
    "        # Similar code could be used to trace entire origin of sequence. For now since server just traces parent of the preceding generation, not needed\n",
    "        # carryover_candidate_parents = candidate_parents.index_select(0, carryover_candidate_indices)  # Not strictly necessary since 1d\n",
    "        # D(carryover_candidate_parents, 'carryover_candidate_parents')\n",
    "\n",
    "        carryover_candidate_logprobs = candidate_logprobs.index_select(0, new_candidate_parents)  # Not strictly necessary since 1d\n",
    "        D(carryover_candidate_logprobs, 'carryover_candidate_logprobs')\n",
    "\n",
    "        new_candidate_toks = sorted_indices[keep_indices].unsqueeze(1)\n",
    "        D(new_candidate_toks, 'new_candidate_toks')\n",
    "        new_candidate_tok_logprobs = sorted_probs[keep_indices].log()\n",
    "        D(new_candidate_tok_logprobs, 'new_candidate_tok_logprobs')\n",
    "\n",
    "        new_candidates = torch.cat([carryover_candidates, new_candidate_toks], dim=1)\n",
    "        D(new_candidates, 'new_candidates')\n",
    "        new_candidate_logprobs = carryover_candidate_logprobs.add_(new_candidate_tok_logprobs)\n",
    "        D(new_candidate_logprobs, 'new_candidate_logprobs')\n",
    "\n",
    "        return new_candidates, new_candidate_parents.tolist(), new_candidate_logprobs\n",
    "\n",
    "\n",
    "    def _infer(self, candidates, candidate_logprobs):\n",
    "        with torch.inference_mode():\n",
    "            num_batches = (candidates.shape[0] + self.batch_size - 1) // self.batch_size  # Round up to nearest whole number of batches\n",
    "            D(num_batches, 'num_batches')\n",
    "\n",
    "            check_gpu('infer start')\n",
    "            output_logits_list = []\n",
    "            output_embeddings_list = []\n",
    "            for i in range(0, num_batches, 1):\n",
    "                batch_candidates = candidates[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "                DS(batch_candidates, 'batch_candidates')\n",
    "                batch_candidate_logprobs = candidate_logprobs[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "                DS(batch_candidate_logprobs, 'batch_candidate_logprobs')\n",
    "\n",
    "                batch_outputs = self.model(input_ids=batch_candidates, output_hidden_states=True)\n",
    "                DS(batch_outputs.logits, 'batch_logits')\n",
    "                DS(batch_outputs.hidden_states[-1], 'hidden_states[-1]')\n",
    "\n",
    "                output_logits_list.append(batch_outputs.logits)\n",
    "                output_embeddings_list.append(batch_outputs.hidden_states[-1][:,-1,:])\n",
    "                check_gpu('infer - after batch run')\n",
    "\n",
    "            output_logits = torch.cat(output_logits_list, dim=0)\n",
    "            output_embeddings = torch.cat(output_embeddings_list, dim=0)\n",
    "            \n",
    "            return output_logits, output_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "669a1b7d-de91-4218-a4b2-64a32b68665c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.ff07dc01615f8113924aed013115ab2abd32115b.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.ff07dc01615f8113924aed013115ab2abd32115b.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb6aac1f23544c1c9cf06d302b5cd71c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing tokenizer...\n",
      "\n",
      "input_ids\n",
      "torch.Size([1, 11])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 32010,  1724,   338,   278,  9939, 14378, 29973, 29871, 32007,\n",
       "         32001]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "num_batches\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infer start: GPU memory used: 14553 MB.\n",
      "\n",
      "batch_candidates\n",
      "torch.Size([1, 11])\n",
      "\n",
      "batch_candidate_logprobs\n",
      "torch.Size([1])\n",
      "\n",
      "batch_logits\n",
      "torch.Size([1, 11, 32064])\n",
      "\n",
      "hidden_states[-1]\n",
      "torch.Size([1, 11, 3072])\n",
      "infer - after batch run: GPU memory used: 14745 MB.\n",
      "FARTHEST NEIGHBORS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "\n",
      "candidates\n",
      "torch.Size([1, 11])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 32010,  1724,   338,   278,  9939, 14378, 29973, 29871, 32007,\n",
       "         32001]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "candidate_logprobs\n",
      "torch.Size([1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "selected\n",
      "torch.Size([1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([True])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "selected_logprobs\n",
      "torch.Size([1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m it \u001b[38;5;241m=\u001b[39m InferenceTensor()\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m it\u001b[38;5;241m.\u001b[39mcandidates_generator(top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, top_p_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.99\u001b[39m, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, max_beams\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhat is the highest mountain?\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(x)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n",
      "Cell \u001b[0;32mIn[16], line 24\u001b[0m, in \u001b[0;36mInferenceTensor.candidates_generator\u001b[0;34m(self, top_p, top_p_decay, top_k, max_beams, max_new_tokens, prompt)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m level_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_new_tokens):\n\u001b[1;32m     22\u001b[0m     logits, embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer(candidates, candidate_logprobs)\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_farthest_neighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidate_logprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_beams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m candidates\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m max_beams:\n\u001b[1;32m     27\u001b[0m         start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n",
      "Cell \u001b[0;32mIn[16], line 119\u001b[0m, in \u001b[0;36mInferenceTensor._farthest_neighbors\u001b[0;34m(self, logits, embeddings, candidates, candidate_logprobs, max_beams)\u001b[0m\n\u001b[1;32m    117\u001b[0m selected_logprobs \u001b[38;5;241m=\u001b[39m candidate_logprobs[selected]\n\u001b[1;32m    118\u001b[0m D(selected_logprobs, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselected_logprobs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 119\u001b[0m selected_logprobs_view \u001b[38;5;241m=\u001b[39m selected_logprobs\u001b[38;5;241m.\u001b[39mview((\u001b[38;5;241m1\u001b[39m, \u001b[43mselected_logprobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m, selected_logprobs[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m    120\u001b[0m D(selected_logprobs_view, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselected_logprobs_view\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    121\u001b[0m d \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcosine_similarity(candidate_logprobs, candidate_logprobs[selected], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "it = InferenceTensor()\n",
    "\n",
    "for x in it.candidates_generator(top_p=0.9, top_p_decay=0.99, top_k=2, max_beams=3, max_new_tokens=6, prompt='What is the highest mountain?'):\n",
    "    print(x)\n",
    "    print()\n",
    "    print('====================================')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
