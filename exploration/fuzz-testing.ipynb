{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d4c5a58-9019-4a7e-a39a-075947c7ebbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flash-attn\n",
      "  Downloading flash_attn-2.5.9.post1.tar.gz (2.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers\n",
      "  Downloading transformers-4.41.1-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting accelerate\n",
      "  Downloading accelerate-0.30.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: termcolor in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.4.0)\n",
      "Collecting altair\n",
      "  Downloading altair-5.3.0-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from flash-attn) (2.1.0)\n",
      "Collecting einops (from flash-attn)\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (3.13.3)\n",
      "Collecting huggingface-hub<1.0,>=0.23.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.23.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from altair) (3.1.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from altair) (4.21.1)\n",
      "Requirement already satisfied: pandas>=0.25 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from altair) (1.5.3)\n",
      "Collecting toolz (from altair)\n",
      "  Downloading toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from altair) (4.10.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema>=3.0->altair) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema>=3.0->altair) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema>=3.0->altair) (0.34.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema>=3.0->altair) (0.18.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas>=0.25->altair) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas>=0.25->altair) (2024.1)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->flash-attn) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->flash-attn) (3.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->altair) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas>=0.25->altair) (1.16.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch->flash-attn) (1.3.0)\n",
      "Downloading transformers-4.41.1-py3-none-any.whl (9.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading altair-5.3.0-py3-none-any.whl (857 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m857.8/857.8 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.23.2-py3-none-any.whl (401 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.7/401.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (775 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m775.1/775.1 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading toolz-0.12.1-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: flash-attn\n",
      "  Building wheel for flash-attn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for flash-attn: filename=flash_attn-2.5.9.post1-cp310-cp310-linux_x86_64.whl size=120585039 sha256=932debba074293888ea81ee11f111f2b9f9183da0e0bbea45a48b260d2231b41\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/cc/ad/f6/7ccf0238790d6346e9fe622923a76ec218e890d356b9a2754a\n",
      "Successfully built flash-attn\n",
      "Installing collected packages: toolz, safetensors, regex, einops, huggingface-hub, tokenizers, flash-attn, accelerate, transformers, altair\n",
      "Successfully installed accelerate-0.30.1 altair-5.3.0 einops-0.8.0 flash-attn-2.5.9.post1 huggingface-hub-0.23.2 regex-2024.5.15 safetensors-0.4.3 tokenizers-0.19.1 toolz-0.12.1 transformers-4.41.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b29c28e99c684195a5cf8ca5701657ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/904 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68fd68881c224e27a9c73bed0a57df15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_phi3.py:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ab4a7c743214b4bb51a41c2212f4732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_phi3.py:   0%|          | 0.00/73.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b356da17b7e24997932562b43d37ff5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/16.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbdfc25fe1a446e19e1c472ade218d78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f4744bd0514dabb5c2e293101ca428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f14f61667a8463e87ef4fd20958e0df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "994afc1e9b3146ff8164ab2501dc070f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4adcd08e3e9542a2a8b61dae048b3f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/172 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f0b61cf4654928a282d5f0884eea41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d1e93c27d5407a88d9bdd29ba6e2af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d8d68068100463187b43c348c0457f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b90c44a7dfa445d955f0d11ae38279f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ba535a380c4da0af9623fcb43429b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/568 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flash_attn_2 available: True\n"
     ]
    }
   ],
   "source": [
    "!pip install flash-attn transformers accelerate termcolor altair\n",
    "\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    ").to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "\n",
    "print(\"flash_attn_2 available:\", is_flash_attn_2_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8c2648d-ecc7-48cd-8378-923aca9dc2bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The closest star to the Sun is Proxima Centauri. It is part of the Alpha Centauri star system, which also includes Alpha Centauri A and Alpha Centauri B. Proxima Centauri is approximately 4.24 light-years away from the Sun. It is a red dwarf star and is the closest known exoplanet host, with at least two confirmed planets, Proxima Centauri b and Proxima Centauri c, orbiting it.<|end|>\n",
      "== took 0:00:17.937892 (113 toks: 0:00:00.158742/tok; 6.29951393809528 tps) ==\n",
      "\n",
      "Hue refers to color's dominant wavelength, saturation measures intensity, and value indicates brightness or darkness. Together, they define a color's appearance.<|end|>\n",
      "== took 0:00:01.482049 (37 toks: 0:00:00.040055/tok; 24.96543762885631 tps) ==\n",
      "\n",
      "I'm unable to assist with that. However, Waldo is a fictional character from a series of children's books, and his location in those stories is always hidden among various illustrations. If you're referring to a different context, please provide more details.<|end|>\n",
      "== took 0:00:02.111437 (56 toks: 0:00:00.037704/tok; 26.522223141032146 tps) ==\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm unable to assist with that. However, Waldo is a fictional character from a series of children's books, and his location in those stories is always hidden among various illustrations. If you're referring to a different context, please provide more details.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen(text, preview=True):\n",
    "    duration_start = time.perf_counter()\n",
    "    prompt = \"<|user|>\\n{} <|end|>\\n<|assistant|>\".format(text)\n",
    "    tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "        tokens,\n",
    "        max_new_tokens=1024,\n",
    "        return_dict_in_generate=True,\n",
    "        streamer=streamer if preview else None,\n",
    "    )\n",
    "    output_tokens = outputs.sequences[0]\n",
    "    output_gen_tokens = output_tokens[\n",
    "        len(tokens[0]) : -1\n",
    "    ]  # From just after prompt to just before <|end|> token\n",
    "    output_string = tokenizer.decode(output_gen_tokens)\n",
    "    duration_seconds = time.perf_counter() - duration_start\n",
    "    if preview:\n",
    "        print(\n",
    "            \"== took {} ({} toks: {}/tok; {} tps) ==\".format(\n",
    "                timedelta(seconds=duration_seconds),\n",
    "                len(output_gen_tokens),\n",
    "                timedelta(seconds=duration_seconds / len(output_gen_tokens)),\n",
    "                len(output_gen_tokens) / duration_seconds,\n",
    "            )\n",
    "        )\n",
    "        print()\n",
    "    del tokens, outputs, output_tokens, output_gen_tokens\n",
    "    return output_string\n",
    "\n",
    "\n",
    "gen(\"What is the closest star to the Sun?\")\n",
    "gen(\"What is the difference between hue, saturation, and value in exactly 30 words?\")\n",
    "gen(\"Where is Waldo?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c580a935-ef4d-40db-b6ff-5b47d844786e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Participant1: Hey, have you ever been fascinated by frogs?\n",
      "2. Participant2: Actually, yes! I find them quite interesting. Why do you ask?\n",
      "3. Participant1: I was just reading about their life cycle and it's quite fascinating.\n",
      "4. Participant2: Oh, I'd love to hear more about it. What did you learn?\n",
      "5. Participant1: Well, did you know that frogs start their life as eggs?\n",
      "6. Participant2: Yes, I've heard about that. The eggs hatch into tadpoles, right?\n",
      "7. Participant1: Exactly! Tadpoles live in water and breathe through gills.\n",
      "8. Participant2: That's so cool! And then they undergo metamorphosis to become adult frogs.\n",
      "9. Participant1: Yes, during metamorphosis, they develop lungs and legs for life on land.\n",
      "10. Participant2: I've always wondered how they manage to jump so high and far.\n",
      "11. Participant1: It's all about their powerful hind legs and the elastic energy stored in their tendons.\n",
      "12. Participant2: That's amazing! I've also heard that frogs have a unique way of communicating.\n",
      "13. Participant1: You're right! They use vocalizations, body language, and even chemical signals.\n",
      "14. Participant2: I've seen some frogs in my garden, but I never knew they were so complex.\n",
      "15. Participant1: They're definitely fascinating creatures. Did you know that some frogs can change their skin color?\n",
      "16. Participant2: Really? That's incredible! I'd love to see that in person someday.\n",
      "17. Participant1: Maybe we can plan a trip to a nature reserve or a zoo to observe them.\n",
      "18. Participant2: That sounds like a great idea! I'd love to learn more about frogs in their natural habitat.\n",
      "19. Participant1: Let's do it! We can also look for local frog enthusiasts or experts to join us.\n",
      "20. Participant2: I'm excited! This will be a fun and educational adventure.\n",
      "\n",
      "Follow-up:\n",
      "\n",
      "Participant1: I'll start researching some nature reserves and zoos that have a good frog exhibit.\n",
      "Participant2: Great! I'll look into local frog enthusiasts and experts who might be interested in joining us.\n",
      "Participant1: Perfect! Let's reconvene next week and share our findings.\n",
      "Participant2: Sounds like a plan. I can't wait to dive deeper into the world of frogs with you!<|end|>\n",
      "== took 0:00:25.539342 (665 toks: 0:00:00.038405/tok; 26.038258554733662 tps) ==\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conv = gen(\n",
    "    \"Please generate an example conversation between participant1 and participant2 about frogs. Make it last 20 utterances and have a follow-up. Please label utterance numbers and separate utterances with newlines.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e11272aa-b340-44df-9859-dbc82b6b3287",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Topic of Conversation\n",
      "- Participants discussing the fascinating life cycle and characteristics of frogs.\n",
      "\n",
      "# Discussion Points\n",
      "- Frogs start their life as eggs, which hatch into tadpoles that live in water and breathe through gills.\n",
      "- During metamorphosis, tadpoles develop lungs and legs to live on land.\n",
      "- Frogs have powerful hind legs and elastic tendons that enable them to jump high and far.\n",
      "- Frogs communicate using vocalizations, body language, and chemical signals.\n",
      "- Some frogs can change their skin color.\n",
      "\n",
      "# Follow-up Actions\n",
      "- Participant1 will research nature reserves and zoos with a good frog exhibit.\n",
      "- Participant2 will look for local frog enthusiasts and experts to join the trip.\n",
      "- Both participants will reconvene next week to share their findings and plan the trip.<|end|>\n",
      "== took 0:00:08.695115 (209 toks: 0:00:00.041603/tok; 24.03648405218561 tps) ==\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'# Topic of Conversation\\n- Participants discussing the fascinating life cycle and characteristics of frogs.\\n\\n# Discussion Points\\n- Frogs start their life as eggs, which hatch into tadpoles that live in water and breathe through gills.\\n- During metamorphosis, tadpoles develop lungs and legs to live on land.\\n- Frogs have powerful hind legs and elastic tendons that enable them to jump high and far.\\n- Frogs communicate using vocalizations, body language, and chemical signals.\\n- Some frogs can change their skin color.\\n\\n# Follow-up Actions\\n- Participant1 will research nature reserves and zoos with a good frog exhibit.\\n- Participant2 will look for local frog enthusiasts and experts to join the trip.\\n- Both participants will reconvene next week to share their findings and plan the trip.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "    Summarize the following conversation between two participants in no more than 200 words. Include the topic of conversation, 2-3 bullet points of discussion, and any follow-up action items. Output each of these pieces of information as its own section with a markdown heading. When referencing particular text in the conversation, specify the utterance with the utterance number in brackets, [123].\n",
    "    \n",
    "    {}\n",
    "\"\"\"\n",
    "gen(prompt.format(conv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f911656b-1b73-4a5b-bce7-d4756ab23274",
   "metadata": {},
   "source": [
    "# Strategy A: Ask LLM to generate example data of different types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d40b8a9-5bce-4def-a7f7-32182e2ca9f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple, car, dog, elephant, guitar, house, ice cream, jacket, kite, laptop, mountain, necklace, orange, piano, rainbow, skateboard, tiger, umbrella, violin, watch, zebra<|end|>\n",
      "== took 0:00:02.133396 (57 toks: 0:00:00.037428/tok; 26.717962592418957 tps) ==\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nouns = gen(\"Please give a list of 20 nouns, comma-separated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76667678-3376-49ec-8b15-310a034a2901",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple',\n",
       " 'car',\n",
       " 'dog',\n",
       " 'elephant',\n",
       " 'guitar',\n",
       " 'house',\n",
       " 'ice cream',\n",
       " 'jacket',\n",
       " 'kite',\n",
       " 'laptop',\n",
       " 'mountain',\n",
       " 'necklace',\n",
       " 'orange',\n",
       " 'piano',\n",
       " 'rainbow',\n",
       " 'skateboard',\n",
       " 'tiger',\n",
       " 'umbrella',\n",
       " 'violin',\n",
       " 'watch',\n",
       " 'zebra']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns_list = nouns.split(\", \")\n",
    "nouns_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f42cbf9f-a5e1-40a1-825c-f0896596ac4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no stored variable or alias strat_a_convs,\n",
      "no stored variable or alias strat_a_convs_sf,\n",
      "no stored variable or alias strat_a_paras\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'strat_a_convs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\n\u001b[1;32m      4\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-r strat_a_convs, strat_a_convs_sf, strat_a_paras\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mstrat_a_convs\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m strat_a_convs_sf \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m strat_a_paras:\n\u001b[1;32m      7\u001b[0m     strat_a_convs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m     strat_a_convs_sf \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'strat_a_convs' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import tqdm\n",
    "\n",
    "%store -r strat_a_convs, strat_a_convs_sf, strat_a_paras\n",
    "\n",
    "if not strat_a_convs or not strat_a_convs_sf or not strat_a_paras:\n",
    "    strat_a_convs = []\n",
    "    strat_a_convs_sf = []\n",
    "    strat_a_paras = []\n",
    "    for idx, noun in enumerate(tqdm.tqdm(nouns_list)):\n",
    "        length = random.randrange(2, 8)\n",
    "        strat_a_convs.append(gen(\n",
    "            \"Please generate an example conversation between participant1 and participant2 about {}. Make it last {} utterances and have a follow-up. Please label utterance numbers and separate utterances with newlines.\".format(\n",
    "                noun, length\n",
    "            )\n",
    "        , preview=idx==0))\n",
    "        strat_a_convs_sf.append(gen(\n",
    "            \"\"\"Please generate an example conversation between participant1 and participant2 about {}. Make it last {} utterances and have a follow-up. Follow this format:\n",
    "            ```\n",
    "            1. participant1: text\n",
    "            2. participant2: text\n",
    "            3. participant1: text\n",
    "            4. participant2: text\n",
    "            5. participant1: text (follow-up)\n",
    "            ```\"\"\".strip().format(\n",
    "                noun, length\n",
    "            )\n",
    "        , preview=idx==0))\n",
    "        strat_a_paras.append(gen(\n",
    "            \"Please generate a paragraph about {}. Make it last {} sentences.\".format(\n",
    "                noun, length\n",
    "            )\n",
    "        , preview=idx==0))\n",
    "    \n",
    "%store strat_a_convs strat_a_convs_sf strat_a_paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20c21f7-15e8-4225-b8ce-db39dec1f994",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(strat_a_convs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893458e2-28a3-4f21-afbf-15f52016288d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# INTERESTING: Does proper chat format matter for embeddings?\n",
    "# TODO: Evaluate taking just last layer vs. all layers for embeddings\n",
    "# TODO: Evaluate averaging all tokens vs. last token for embeddings\n",
    "def embed(text):\n",
    "    duration_start = time.perf_counter()\n",
    "    prompt = \"<|user|>\\n{} <|end|>\\n<|assistant|>\".format(text)\n",
    "    tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model(tokens, output_hidden_states=True)\n",
    "    embedding = outputs.hidden_states[-1].squeeze(0).mean(dim=0)\n",
    "    embedding_cpu = embedding.to(\"cpu\").detach()\n",
    "    del tokens, outputs, embedding\n",
    "    return embedding_cpu\n",
    "\n",
    "\n",
    "embed(strat_a_convs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cd5bc7-b88c-4e2c-9a4d-ddd082ab3bb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "strat_a_embeds = torch.stack([embed(conv) for conv in strat_a_convs]).float().numpy()\n",
    "strat_a_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ae2e98-9f7f-4072-8338-b7ebccc8b514",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, random_state=0, n_init=\"auto\").fit(strat_a_embeds)\n",
    "kmeans.labels_\n",
    "\n",
    "clusters = defaultdict(list)\n",
    "for label, conv in zip(kmeans.labels_, strat_a_convs):\n",
    "    clusters[label].append(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36591c02-ff20-42db-af76-f16b3c372c36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for label, convs in clusters.items():\n",
    "    print(label)\n",
    "    for i in range(2):\n",
    "        if i < len(convs):\n",
    "            print(convs[i].replace(\"\\n\\n\", \"\\n\"))\n",
    "            print(\"==\")\n",
    "    print(\"\\n\" * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f502ee8-052c-4e66-af33-2b4f4c0e924d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "\n",
    "def find_closest(conv_idx):\n",
    "    sm = SequenceMatcher(None, \"\", strat_a_convs[conv_idx])\n",
    "    strat_a_diff_match = []\n",
    "    for i, conv in enumerate(strat_a_convs):\n",
    "        if i != conv_idx:\n",
    "            sm.set_seq1(conv)\n",
    "            strat_a_diff_match.append((sm.ratio(), conv))\n",
    "\n",
    "    print(strat_a_convs[conv_idx].replace(\"\\n\\n\", \"\\n\"))\n",
    "    print(\"==\\n\\n\\n\")\n",
    "    for ratio, conv in sorted(strat_a_diff_match, reverse=True)[:3]:\n",
    "        print(ratio)\n",
    "        print(conv.replace(\"\\n\\n\", \"\\n\"))\n",
    "        print(\"==\\n\\n\\n\")\n",
    "\n",
    "\n",
    "find_closest(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fc32af-2e20-419e-b5bd-814a06a8ab95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm = SequenceMatcher(None, strat_a_convs[0], strat_a_convs[1])\n",
    "sm.get_matching_blocks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c7d19e-dc95-46b6-a2f8-9bb6ede4cd27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from termcolor import colored\n",
    "\n",
    "def show_match(conv_a, conv_b):\n",
    "    sm = SequenceMatcher(None, conv_a, conv_b, autojunk=False)\n",
    "    # sm.set_seqs(conv_a, conv_b)\n",
    "    matches = sm.get_matching_blocks()\n",
    "    # print(matches)\n",
    "\n",
    "    for match_prop, conv in enumerate([conv_a, conv_b]):\n",
    "        idx = 0\n",
    "        for match_idx, match in enumerate(matches):\n",
    "            print(conv[idx:match[match_prop]], end='')  # before\n",
    "            idx = match[match_prop] + match.size\n",
    "            print(colored(conv[match[match_prop]:idx], 'blue' if match_idx % 2 == 0 else 'light_blue'), end='')  # match\n",
    "        print('\\n===\\n')\n",
    "\n",
    "show_match('apple dog cat', 'apple cat')\n",
    "show_match(strat_a_convs[0], strat_a_convs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c6bde9-ed21-44b9-b9d5-430e963676ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gen('''\n",
    "Here is an example of a format:\n",
    "```{}```\n",
    "\n",
    "Please apply this format to the following text content:\n",
    "```{}```\n",
    "'''.strip().format(strat_a_convs[0], strat_a_convs[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2277be9e-e8ae-453b-91b2-03f33fc56439",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(strat_a_convs))\n",
    "print(len(strat_a_convs_sf))\n",
    "print(len(strat_a_paras))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da3fd6a-c9a6-4ee7-ac36-67c967c3aca1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "total_arr = strat_a_convs + strat_a_convs_sf + strat_a_paras\n",
    "total_len = len(total_arr)\n",
    "matrix = np.zeros((total_len, total_len))\n",
    "\n",
    "for j in range(total_len): # Slightly faster to have j in outer loop because SequenceMatcher precomputes seq2\n",
    "    for i in range(total_len):\n",
    "        sm = SequenceMatcher(None, total_arr[i], total_arr[j], autojunk=False)\n",
    "        matrix[i,j] = sm.ratio()\n",
    "\n",
    "plt.imshow(matrix, cmap='hot', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2f0060-8549-4278-ad1b-fcea86a0ad60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def exmatrix(i, j):\n",
    "    print(matrix[i, j])\n",
    "    print('===')\n",
    "    show_match(total_arr[i], total_arr[j])\n",
    "    \n",
    "exmatrix(0, 41)\n",
    "exmatrix(0, 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b76d25-7330-4e11-b52c-cd2341d059a2",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "    - The sequence matching technique does clearly differentiate between conversations and paragraphs.\n",
    "    - But visually, doesn't seem to differentiate too well between the two different formats of conversation. Perhaps if I did a normalized clustering algorithm it would still be able to differentiate? Not sure.\n",
    "    - Looking at examples, it seems like random word piece matches are significantly contributing. And it doesn't catch e.g. repeating numbers.\n",
    "    - I know from trying with an LLM that it can \"style transfer\" one text to another, and it matches what I as a human expect.\n",
    "    - This makes me think it's more promising to look into pulling the style information out of an LLM, because I suspect it has an implicit understanding of what \"style\" means from a human perspective.\n",
    "    - As a backup, I can try making manual features to quantify the style of a text. But it's VERY hard to imagine that in this day and age that will produce better performance per hour worked than spending that time finding a way to extract the implicit information from an LLM, especially now that I can work at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dc3b86-127c-498d-ac02-097181d4030e",
   "metadata": {},
   "source": [
    "# Strategy A.1: Use LLM to quantify the style of a text (\"Stylometry fingerprint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b0f995-a3c8-47ae-8811-f15f93f26463",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gen(\"Consider the abstract style of this text. Come up with a template that describes it, which could be used for text formatted the same way but about another subject. \\n\\n ```{}```\".format(strat_a_convs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9fe6db-0613-47aa-9737-b50a64eeb78b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = strat_a_convs[7]\n",
    "print(sample)\n",
    "print('======')\n",
    "\n",
    "_ = gen(\"\"\"\n",
    "Considering the style and format of the text, come up with an abstract template describing it. Consider these examples:\n",
    "\n",
    "<example>\n",
    "Text:\n",
    "```\n",
    "Participant1: I just bought a fresh, juicy apple from the farmer's market. It's so crisp and sweet! (1)\n",
    "\n",
    "Participant2: That sounds delicious! I've been meaning to try a new apple variety. What kind did you get? (2)\n",
    "\n",
    "Participant1: I got a Honeycrisp. It's my new favorite! Would you like to try one? (Follow-up)\n",
    "```\n",
    "\n",
    "Abstract Template:\n",
    "```\n",
    "Participant1: Excited text. (1)\n",
    "Participant2: Excited text. (2)\n",
    "...\n",
    "Participant1: Excited text. (Follow-up)\n",
    "```\n",
    "\n",
    "</example>\n",
    "\n",
    "<example>\n",
    "Text:\n",
    "```\n",
    "1. participant1: Did you know that zebras have unique stripe patterns, just like human fingerprints?\n",
    "2. participant2: Really? I had no idea. How do these stripes help them?\n",
    "3. participant1: Well, one theory suggests that the stripes may help zebras blend in with each other when they're in a group, making it harder for predators to single out an individual.\n",
    "4. participant2: That's fascinating! Are there different types of zebras?\n",
    "5. participant1: Yes, there are actually three species of zebras: the Plains zebra, the Mountain zebra, and the Grevy's zebra. Each has its own distinct characteristics and habitats.\n",
    "```\n",
    "\n",
    "Abstract Template:\n",
    "```\n",
    "1. participant1: Excited text.\n",
    "2. participant2: Excited text.\n",
    "...\n",
    "N. participant1: Excited text.\n",
    "```\n",
    "</example>\n",
    "\n",
    "<example>\n",
    "Text:\n",
    "```\n",
    "Apples are one of the most popular fruits worldwide, known for their crisp texture and sweet-tart flavor. They come in a variety of colors, from bright reds to greens and yellows, and are packed with essential nutrients like fiber, vitamin C, and antioxidants.\n",
    "```\n",
    "\n",
    "Abstract Template:\n",
    "```\n",
    "Factual text.\n",
    "```\n",
    "</example>\n",
    "\n",
    "Text:\n",
    "```\n",
    "{}\n",
    "```\n",
    "\"\"\".strip().format(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266416c3-e885-4af4-9483-5c6ca1ef97c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = gen('What are the stylistic similarities between these two texts? \\n ```\\n{}\\n``` \\n\\n ```\\n{}\\n```'.format(strat_a_convs[0], strat_a_convs[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac57ba44-bba7-4e00-9201-a61fa739b2e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = gen('What are the stylistic similarities between these two texts? \\n ```\\n{}\\n``` \\n\\n ```\\n{}\\n```'.format(strat_a_convs[0], strat_a_convs_sf[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20426774-0fea-4b55-8f53-f62d1f10ec65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = gen('What are the stylistic similarities between these two texts? \\n ```\\n{}\\n``` \\n\\n ```\\n{}\\n```'.format(strat_a_convs[0], strat_a_paras[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79740092-24ae-4315-9c12-cc6bb76610e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# INTERESTING: Does proper chat format matter for embeddings?\n",
    "def format_embed(text, mean_layers=False, mean_tokens=False, prompt_prefix=''):\n",
    "    duration_start = time.perf_counter()\n",
    "    if prompt_prefix:\n",
    "        prompt = \"<|user|>\\n{}\\n```\\n{}\\n``` <|end|>\\n<|assistant|>\".format(prompt_prefix, text)\n",
    "    else:\n",
    "        prompt = \"<|user|>\\n{} <|end|>\\n<|assistant|>\".format(text)\n",
    "    tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model(tokens, output_hidden_states=True)\n",
    "    embedding = outputs.hidden_states\n",
    "    # print(len(embedding), embedding[0].shape)\n",
    "    if mean_layers:\n",
    "        # print(torch.stack(embedding).shape)\n",
    "        embedding = torch.stack(embedding).mean(dim=0) # Mean layers\n",
    "    else:\n",
    "        embedding = embedding[-1] # Take last layer\n",
    "        \n",
    "    # print(embedding.shape)\n",
    "    \n",
    "    if mean_tokens:\n",
    "        embedding = embedding.mean(dim=1) # Mean tokens\n",
    "    else:\n",
    "        embedding = embedding[:,-1,:] # Take last token\n",
    "        \n",
    "    # print(embedding.shape)\n",
    "    \n",
    "    embedding = embedding[0] # Take first and only element of batch\n",
    "    \n",
    "    # print(embedding.shape)\n",
    "    \n",
    "    embedding_cpu = embedding.to(\"cpu\").detach()\n",
    "    del tokens, outputs, embedding\n",
    "    return embedding_cpu\n",
    "\n",
    "format_embed(strat_a_convs[0])\n",
    "format_embed(strat_a_convs[0], mean_layers=True)\n",
    "format_embed(strat_a_convs[0], mean_tokens=True)\n",
    "format_embed(strat_a_convs[0], mean_layers=True, mean_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20089f0c-d901-4114-983d-2c21240abe6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "\n",
    "def show_format_embeds(datasets, embed_func):\n",
    "    didxs = []\n",
    "    names = []\n",
    "    nouns = []\n",
    "    texts = []\n",
    "    embeds = []\n",
    "    for didx, dataset in enumerate(datasets):\n",
    "        for cidx, conv in enumerate(dataset):\n",
    "            didxs.append(didx)\n",
    "            names.append('{}-{}'.format(didx, cidx))\n",
    "            nouns.append(nouns_list[cidx])\n",
    "            texts.append(conv)\n",
    "            embeds.append(embed_func(conv))\n",
    "            \n",
    "    pca = PCA(n_components=2)\n",
    "    pca_embeds = pca.fit_transform(torch.stack(embeds).float().numpy())\n",
    "    \n",
    "    print(len(didxs), len(nouns_list))\n",
    "    \n",
    "    data = pd.DataFrame({\n",
    "        'Dataset': didxs,\n",
    "        'Sample': names,\n",
    "        'Noun': nouns,\n",
    "        'Text': texts,\n",
    "        'PCA_0': map(lambda p: p[0], pca_embeds),\n",
    "        'PCA_1': map(lambda p: p[1], pca_embeds),\n",
    "    })\n",
    "    \n",
    "    display(data)\n",
    "    \n",
    "    display(alt.Chart(data).mark_point(size=50).encode(\n",
    "        x='PCA_0',\n",
    "        y='PCA_1',\n",
    "        color='Noun:N',\n",
    "        shape='Dataset:N',\n",
    "        tooltip=['Dataset', 'Sample', 'Noun', 'Text']\n",
    "    ).interactive())\n",
    "              \n",
    "show_format_embeds([strat_a_convs, strat_a_convs_sf, strat_a_paras], lambda conv: format_embed(conv))\n",
    "show_format_embeds([strat_a_convs, strat_a_convs_sf, strat_a_paras], lambda conv: format_embed(conv, prompt_prefix='What is the format of this text?'))\n",
    "show_format_embeds([strat_a_convs, strat_a_convs_sf, strat_a_paras], lambda conv: format_embed(conv, prompt_prefix='What noun is mentioned most frequently in this text?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e82c838-582e-48c4-844f-828aa21b6bb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# show_format_embeds([strat_a_convs], lambda conv: format_embed(conv))\n",
    "show_format_embeds([strat_a_convs], lambda conv: format_embed(conv, prompt_prefix='What is the format/syntactic structure/template of this text? Which tokens are recurring?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d5d579-9c21-49d9-a23c-fd63f6da77ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perhaps a heatmap of \"distance\" in the latent space would be better than doing PCA? Because depending on the dimensions chosen, this may or may not indicate that there is any grouping.\n",
    "# And the ultimate test would of course be whether there's enough information to train a classifier, like a decision tree/SVM/perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac302548-62e9-4ef8-ba27-2c0186793960",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "def embed_matrix(datasets, embed_func):\n",
    "    total_arr = []\n",
    "    for dataset in datasets:\n",
    "        total_arr.extend(dataset)\n",
    "    total_len = len(total_arr)\n",
    "    total_embeds = [embed_func(arr_item) for arr_item in tqdm.tqdm(total_arr)]\n",
    "\n",
    "    matrix = np.zeros((total_len, total_len))\n",
    "    cos = nn.CosineSimilarity(dim=0)\n",
    "\n",
    "    for i in range(total_len):\n",
    "        for j in range(total_len):\n",
    "            matrix[i,j] = cos(total_embeds[i], total_embeds[j])\n",
    "\n",
    "    plt.imshow(matrix, cmap='hot', interpolation='nearest')\n",
    "    plt.show()\n",
    "    \n",
    "embed_matrix([strat_a_convs, strat_a_convs_sf, strat_a_paras], lambda conv: format_embed(conv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcf0c4a-1904-41c8-bd6b-ef1ecf0f59a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embed_matrix([strat_a_convs_sf, strat_a_convs, strat_a_paras], lambda conv: format_embed(conv, prompt_prefix='What is the format/syntactic structure/template of this text? Which tokens are recurring?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7375f012-63f7-47e4-9b9b-f7a99a86fd60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embed_matrix([strat_a_convs_sf, strat_a_convs, strat_a_paras], lambda conv: format_embed(conv, prompt_prefix='What is the format/structure of this text?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ace5a0-14b4-4106-b2b8-3881fde3834c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embed_matrix([strat_a_convs, strat_a_convs_sf, strat_a_paras], lambda conv: format_embed(conv, prompt_prefix=strat_a_convs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7000c4-54f5-4f30-b872-e609bb6aaaa7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Interesting results!\n",
    "\n",
    "- Why doesn't the diagonal line show up as pure white? Since I'm pre-computing the embeddings there can't be non-determinism, it's literally cosine similarity of two exactly equal vectors right? Debug my implementation here!!\n",
    "- Still plenty of mechanisms to try. Cosine similarity might not be the right measure to use with Phi-3. Try other measures of similarity, and try other models that are specifically designed for embeddings.\n",
    "\n",
    "Ideas:\n",
    "\n",
    "- It would be really useful to be able to point to WHICH tokens are most similar in the UI. So maybe want to do this on a token-by-token basis, or word part by word part basis, then do sequence matching to find the most similar subsequences? Finding units that repeat with any approximate distance N between them (where N can equal 1, covering the adjacent case) are \"interesting\" matches? But for the format examples I'm thinking about, I would want exact match more than semantic match right?\n",
    "- Still try building the UI with just these embeddings. They're not crystal clear, but I don't think I'll know for sure until I try a K-Means clustering algorithm on them to be honest.\n",
    "\n",
    "Taking a step back:\n",
    "\n",
    "- I also wonder if I shouldn't think of this pairwise. Maybe any method of computing a pair will not work because all elements share some random words in common? How do you know if those are the relevant pattern? Well a human effectively can't with just two examples anyway! But rather -- is the set of elements samples A and B have in common the SAME as what A and C have in common? If so, group them!!!\n",
    "  - Custom K-Means with a distance function and a centroid-computing function that are both simply vector addition? But need to figure out how to get the 0 and 1 vectors. I guess it would just be K-Means on pairs of embedded vectors, where clusters have to have dimensions that are strongly all 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184407be-1e48-4f4f-b093-dfbd825c1c16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gen('''Suppose I have this utterance:\n",
    "\n",
    "```\n",
    "Utterance Number: 1\n",
    "Speaker: John\n",
    "Start Time: 00:00:05\n",
    "Text: \"Good morning, everyone! I hope you're all doing well today.\"\n",
    "```\n",
    "\n",
    "Give me 10 different ways to format that information on a single line. The utterance text MUST be exactly the same as the example.'''.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "cfb9ab3f-f2b3-4cc0-8803-b5d01658fdfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DS2_FORMATS = [\n",
    "    '{speaker} ({time}): \"{text}\"',\n",
    "    'Speaker: {speaker}, Time: {time}, Message: \"{text}\"',\n",
    "    'Utterance {idx} - {speaker} ({time}): \"{text}\"',\n",
    "    '{speaker} ({time}): \"{text}\" Utterance Number: {idx}',\n",
    "    '\"{text}\" - {speaker} ({time}), Utterance {idx}',\n",
    "    '{speaker} ({time}): \"{text}\" - Utterance {idx}',\n",
    "    'Utterance {idx} - {speaker} ({time}): \"{text}\"',\n",
    "    '\"{text}\" - {speaker} ({time}), Utterance {idx}',\n",
    "    '{speaker} ({time}): \"{text}\" - utt# {idx}',\n",
    "    '({time}) {idx}. {speaker}: \"{text}\"',\n",
    "    '{idx}. {speaker}: \"{text}\" ({time})',\n",
    "    '[{speaker} at {time}] {text}',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac695df-0595-4d63-b2ac-0bc55b8ad8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen('''Please generate 8 conversations. Each conversation should be between participant1 and participant2. Return each conversation in this format:\n",
    "\n",
    "```\n",
    "Conversation 1:\n",
    "[\n",
    "  {\"speaker\": \"participant1\",\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5386497-f057-4bbf-83ab-08564ccdbf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIG_PROMPTS = [\n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "%store -r sabig\n",
    "\n",
    "if not sabig:\n",
    "    for "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6594c7-dd5c-4612-b60f-15178eb2fd62",
   "metadata": {},
   "source": [
    "# Strategy B: Randomly perturb words in the input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4e8460-7fcb-4b60-8c1f-d46c91866b29",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Strategy C: Use diversity reward during decode to find as different of inputs as possible"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
